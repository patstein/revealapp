{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sevi/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading package punkt to /home/sevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "#NLP Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Embeddings\n",
    "import gensim\n",
    "#from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_by_sentence(text, stopwords):\n",
    "    '''creates a list of list where each list contains the tokens of a sentence.'''\n",
    "    list_all = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        curr_sent = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if(word in stopwords):\n",
    "                continue\n",
    "            curr_sent.append(word.lower())\n",
    "        list_all.append(curr_sent)\n",
    "    return list_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = '''I opened up a money market account at First Niagara Bank in XXXX XXXX CT on XXXX/XXXX/2016. I was told that the APY interest rate was 1.01 % based on the advertisement in the XXXX XXXX as well as a verbal discussion. There was no mention that the rate was related to compounding and this was confirmed verbally with the bank officer. And that taking out the interest monthly would not affect the rate that I was to receive. After 33 days of \" interest '', from XX/XX/XXXX to XX/XX/XXXX, I received not XXXX of 1.01 % but rather XXXX of 0.81 % and my printed statement says that my APY is now 0.81 % not the 1.01 % I was expecting. \n",
    "The interest I received was XXXX % LESS that I was expecting. The difference in the amount of money is not very much but First Niagara, in my opinion, lied to me with false advertisements and false verbal discussions, apparently to get me to put my money into a seemingly high interest money market account. Multiply this by XXXX customers and XXXX dollars and the amount of money they defrauded consumers could be very substantial. In my opinion, First Niagara deceived me and its customers and violated honest banking practices.\n",
    "Product: Bank account or service'''\n",
    "doc_2 = '''In this post we covered different approaches for word representation in NLP tasks (BOW, TF-IDF and Word Embeddings), learnt how to learn word representation from its context using Word2Vec, saw how we can extract meaningful phrases from a given corpus (NPMI and data-driven approach) and how to transform a given corpus in order to learn similar terms/words for each one of extracted terms/words using Word2Vec algorithm. The results of this process can be used in a downstream task, like Query Expansion in Information Extraction tasks, Document Classification, Clustering, Question-Answering and many more.'''\n",
    "doc_3 = '''On our 1.6 billion words corpus, it took us 1 hour to construct bi-grams and another 2 hours to train Word2Vec (with batch Skip-Gram, 300 dimension, 10 epochs, context of k=5 , negative sampling of 5, learning rate of 0.01 and minimum word count of 5) on a machine with 16 CPUs and 64 RAM using AWS Sagemaker service. A great Notebook example of how to use AWS Sagemaker service to train Word2Vec can be found here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = pd.DataFrame(data = [doc_1, doc_2, doc_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I opened up a money market account at First Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this post we covered different approaches f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On our 1.6 billion words corpus, it took us 1 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  I opened up a money market account at First Ni...\n",
       "1  In this post we covered different approaches f...\n",
       "2  On our 1.6 billion words corpus, it took us 1 ..."
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating AdHoc Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = sample_text.apply(lambda row: tokenize_text_by_sentence(row[0], STOPWORDS), axis=1)# tokenize_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = list(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sent_tokens for sent_tokens_lists in text_tokens for sent_tokens in sent_tokens_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-b7908ccb6ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "embedding = Word2Vec(sentences = corpus, size=100, window=20, min_count=500, workers=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Doc2Vec(dm = 0, vector_size = 300, negative = 5, hs = 0, min_counts=2, smple = 0, workers = -1)\n",
    "embedding.build_vocab(text_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.77278673e-04, -1.15563883e-03,  4.94539679e-04,  5.29558929e-05,\n",
       "        2.18967296e-04,  1.49480195e-03,  7.89466212e-05,  1.36261294e-03,\n",
       "        1.11570305e-04,  1.13489499e-04, -1.54260604e-03, -2.38149369e-04,\n",
       "        1.11785193e-03,  3.84020444e-04, -1.49921223e-04,  7.44852354e-04,\n",
       "       -1.32256898e-03,  1.31975336e-03, -1.39877651e-04, -2.03911564e-04,\n",
       "        4.91718354e-04, -2.58305139e-04,  1.56594062e-04,  9.44201718e-04,\n",
       "        1.52919535e-03,  2.48646364e-04,  1.17718871e-03,  1.23605854e-03,\n",
       "       -1.19386788e-03, -1.55568845e-03,  2.60521221e-04,  5.16726868e-04,\n",
       "        5.61731635e-04,  7.92611914e-04, -1.36623066e-03, -6.13164855e-04,\n",
       "        1.55122380e-03, -1.04656722e-03, -1.53520537e-04, -7.36722068e-05,\n",
       "       -1.01118302e-03, -1.78377566e-04,  1.33298896e-03,  7.51186919e-04,\n",
       "       -1.38469611e-03, -1.49988034e-03,  2.76544451e-04,  2.35665590e-04,\n",
       "        1.08453794e-03, -1.19828049e-03, -5.81086788e-04,  1.07764173e-03,\n",
       "        4.44321922e-04,  9.28916386e-04, -1.51679583e-03, -5.88845811e-04,\n",
       "       -8.96233905e-05,  1.20396458e-03, -1.11958955e-03, -8.36065272e-04,\n",
       "        3.30643670e-04, -1.41124474e-05, -3.61790328e-04, -4.97040979e-04,\n",
       "       -1.43859722e-03,  5.42817404e-04, -3.32568190e-04, -1.60602608e-03,\n",
       "       -9.24664666e-04, -8.44837487e-05, -1.21025858e-03, -5.11903490e-04,\n",
       "       -5.67360548e-04,  1.16283505e-03,  4.72179468e-04,  2.47354299e-04,\n",
       "       -1.59710500e-04, -9.60653415e-05, -1.68477665e-04, -1.32987194e-03,\n",
       "       -1.06042507e-03,  1.57963473e-03,  1.29425887e-03,  9.16944584e-04,\n",
       "       -1.64517167e-03, -5.81860426e-04, -8.09668738e-04,  6.45532447e-04,\n",
       "        4.36076720e-04,  4.35383787e-04, -5.70986886e-04, -3.54268966e-04,\n",
       "        1.17192243e-03,  2.50427081e-04,  1.29895168e-04, -1.55125698e-03,\n",
       "       -1.48238370e-03, -5.24924020e-04,  8.22400500e-04,  1.12143194e-03,\n",
       "       -4.93991014e-04,  1.49323943e-03, -1.01126287e-04, -1.20311463e-03,\n",
       "       -3.16223130e-04, -1.34364783e-03, -8.05113639e-04, -1.31220848e-03,\n",
       "        1.48383889e-03,  1.47599634e-03,  1.48638722e-03,  5.76554623e-04,\n",
       "        1.12577621e-03,  9.55257157e-04,  6.93065056e-04, -1.26240845e-03,\n",
       "       -1.02755718e-03, -1.08249288e-03, -1.36264821e-03,  1.10193761e-03,\n",
       "        1.15214614e-03, -6.66264095e-04, -1.03037234e-03,  2.37883360e-04,\n",
       "       -1.07022031e-04, -3.84489162e-04,  1.07342843e-03, -1.03046675e-03,\n",
       "       -1.65592891e-03, -1.10432785e-03,  1.22680573e-03, -1.20743643e-03,\n",
       "       -3.49625596e-04,  6.32685376e-04, -4.69952880e-04, -6.68738096e-04,\n",
       "       -4.71871346e-04, -1.63530675e-03, -1.10969867e-03, -2.48028402e-04,\n",
       "       -1.17242616e-03, -5.75010956e-04, -7.46959879e-04,  1.03750511e-03,\n",
       "       -1.87510916e-04,  5.08202473e-04, -5.34375198e-04, -1.86530218e-04,\n",
       "        7.91790255e-04, -6.59253157e-04,  1.44354440e-03,  1.51369546e-03,\n",
       "       -4.56253940e-04, -1.38114952e-03, -1.11831643e-03, -6.26282475e-04,\n",
       "        7.09587592e-04,  1.35432405e-03,  5.88755007e-04,  6.69781293e-04,\n",
       "       -1.57913449e-03,  1.60853355e-03,  1.23481022e-03, -1.03184616e-03,\n",
       "        1.62834418e-04,  6.23648288e-04, -1.05668593e-03,  8.61006440e-04,\n",
       "        8.83505738e-04,  4.97044821e-04,  1.37129310e-03,  1.10302586e-03,\n",
       "       -1.51596870e-03,  1.34248100e-03,  1.24771171e-03, -6.36288954e-04,\n",
       "        8.51610792e-04,  1.58466492e-03,  2.86900962e-04, -3.40979605e-04,\n",
       "        1.37324666e-03,  8.86644993e-04,  8.84019013e-04,  6.21927320e-04,\n",
       "        8.29083670e-04, -3.04736623e-05,  1.31860340e-03,  9.55179974e-04,\n",
       "       -1.27267689e-04, -9.99901211e-04,  7.19415024e-04, -3.54010350e-04,\n",
       "       -1.60856650e-03, -1.26045104e-03, -1.58435921e-03, -1.59663043e-03,\n",
       "       -3.31410760e-04, -1.41527166e-03, -1.46265794e-03, -3.73252842e-04,\n",
       "       -1.47284963e-03, -2.81177083e-04, -1.56333041e-03, -1.29339809e-03,\n",
       "       -1.05215360e-04,  7.81754090e-04, -3.47228961e-05,  1.03183382e-03,\n",
       "       -1.04123796e-03, -1.84793040e-04, -1.39294902e-03,  1.33931369e-03,\n",
       "        1.58675411e-03,  2.60543253e-04, -1.61343697e-03,  1.35655294e-03,\n",
       "       -9.23554122e-04,  5.98151586e-04, -4.60098236e-04, -6.98516960e-04,\n",
       "        1.32024579e-03,  5.91942866e-04,  8.53080070e-04, -6.53685303e-04,\n",
       "        1.06192532e-03, -6.12189062e-04,  8.98544153e-04, -1.28266052e-03,\n",
       "        3.42026731e-04,  1.23153336e-03, -4.81011288e-04, -1.48749922e-03,\n",
       "        3.86263127e-04,  1.15254588e-04,  1.12556934e-03,  1.37234339e-04,\n",
       "       -1.17331347e-03, -8.87732487e-04, -1.02477369e-03,  1.10231875e-03,\n",
       "        1.24069827e-03, -5.27135737e-04, -9.55411058e-04,  1.08169438e-03,\n",
       "        1.50940707e-03, -1.23274556e-04,  1.51401083e-03, -5.72586665e-04,\n",
       "        2.23410214e-04,  6.73052215e-04,  1.63713389e-03, -1.05846568e-03,\n",
       "       -9.73133661e-04,  1.89564555e-04, -1.47725956e-03,  5.86702547e-04,\n",
       "        4.33760724e-05,  1.29364227e-04,  6.51396462e-04,  1.60225178e-03,\n",
       "        1.48130045e-03,  1.22536730e-03, -4.93838335e-04, -2.03613658e-04,\n",
       "       -7.70287952e-05,  2.87381437e-04,  5.39796929e-05, -7.52769120e-04,\n",
       "        6.63103710e-04, -1.19781937e-03,  1.27450749e-03, -1.21475873e-03,\n",
       "        4.71265404e-04,  1.05195900e-03,  4.73633350e-04, -7.32158776e-04,\n",
       "       -1.42917025e-03,  2.44674826e-04,  1.26867054e-03,  1.27716921e-03,\n",
       "       -9.43136634e-04, -8.88592214e-04,  7.86402030e-04, -2.37570712e-04,\n",
       "       -6.99633791e-04, -1.16549048e-03,  8.25511233e-04, -5.24693402e-04,\n",
       "        2.63187074e-04,  1.51036063e-03, -1.04170013e-03, -1.05211989e-03,\n",
       "       -8.24676070e-04, -8.30515564e-05,  1.15368610e-04, -1.81460331e-04,\n",
       "        8.16518732e-04, -1.30909961e-03, -3.08485090e-04,  1.54767616e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.infer_vector(['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This notebook assumes a function (the function created by Patrick) that outputs for a given tag all the word vectors that are most similar'''\n",
    "word_space_dim = 100\n",
    "number_of_words = 500\n",
    "fake_embedding = pd.DataFrame(np.random.rand(word_space_dim, number_of_words))\n",
    "#df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.927145</td>\n",
       "      <td>0.556246</td>\n",
       "      <td>0.955109</td>\n",
       "      <td>0.995895</td>\n",
       "      <td>0.580252</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.367849</td>\n",
       "      <td>0.102461</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.552692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345356</td>\n",
       "      <td>0.451318</td>\n",
       "      <td>0.610774</td>\n",
       "      <td>0.936466</td>\n",
       "      <td>0.272332</td>\n",
       "      <td>0.052199</td>\n",
       "      <td>0.633744</td>\n",
       "      <td>0.020181</td>\n",
       "      <td>0.474952</td>\n",
       "      <td>0.728678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.785615</td>\n",
       "      <td>0.903626</td>\n",
       "      <td>0.161089</td>\n",
       "      <td>0.109162</td>\n",
       "      <td>0.710021</td>\n",
       "      <td>0.970078</td>\n",
       "      <td>0.934077</td>\n",
       "      <td>0.984116</td>\n",
       "      <td>0.092469</td>\n",
       "      <td>0.922313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084911</td>\n",
       "      <td>0.556243</td>\n",
       "      <td>0.226116</td>\n",
       "      <td>0.090481</td>\n",
       "      <td>0.392536</td>\n",
       "      <td>0.821097</td>\n",
       "      <td>0.255073</td>\n",
       "      <td>0.535103</td>\n",
       "      <td>0.523670</td>\n",
       "      <td>0.785093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.699413</td>\n",
       "      <td>0.831009</td>\n",
       "      <td>0.930310</td>\n",
       "      <td>0.731158</td>\n",
       "      <td>0.069717</td>\n",
       "      <td>0.735959</td>\n",
       "      <td>0.918918</td>\n",
       "      <td>0.773546</td>\n",
       "      <td>0.170292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840504</td>\n",
       "      <td>0.806163</td>\n",
       "      <td>0.932701</td>\n",
       "      <td>0.988374</td>\n",
       "      <td>0.341064</td>\n",
       "      <td>0.262697</td>\n",
       "      <td>0.091428</td>\n",
       "      <td>0.876601</td>\n",
       "      <td>0.806742</td>\n",
       "      <td>0.863439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.992364</td>\n",
       "      <td>0.653169</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.280821</td>\n",
       "      <td>0.751770</td>\n",
       "      <td>0.410553</td>\n",
       "      <td>0.246366</td>\n",
       "      <td>0.652241</td>\n",
       "      <td>0.486327</td>\n",
       "      <td>0.347260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764963</td>\n",
       "      <td>0.698155</td>\n",
       "      <td>0.616626</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.835469</td>\n",
       "      <td>0.283578</td>\n",
       "      <td>0.389541</td>\n",
       "      <td>0.925172</td>\n",
       "      <td>0.602922</td>\n",
       "      <td>0.701857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.698501</td>\n",
       "      <td>0.435902</td>\n",
       "      <td>0.843922</td>\n",
       "      <td>0.259721</td>\n",
       "      <td>0.797488</td>\n",
       "      <td>0.972315</td>\n",
       "      <td>0.269579</td>\n",
       "      <td>0.798582</td>\n",
       "      <td>0.500263</td>\n",
       "      <td>0.250349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674325</td>\n",
       "      <td>0.473022</td>\n",
       "      <td>0.449506</td>\n",
       "      <td>0.485336</td>\n",
       "      <td>0.349590</td>\n",
       "      <td>0.536662</td>\n",
       "      <td>0.849283</td>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.666335</td>\n",
       "      <td>0.238990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.778028</td>\n",
       "      <td>0.701222</td>\n",
       "      <td>0.303444</td>\n",
       "      <td>0.113863</td>\n",
       "      <td>0.616086</td>\n",
       "      <td>0.370898</td>\n",
       "      <td>0.141494</td>\n",
       "      <td>0.655690</td>\n",
       "      <td>0.268150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185427</td>\n",
       "      <td>0.651085</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.269433</td>\n",
       "      <td>0.933416</td>\n",
       "      <td>0.652839</td>\n",
       "      <td>0.143622</td>\n",
       "      <td>0.074453</td>\n",
       "      <td>0.624741</td>\n",
       "      <td>0.124699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.124268</td>\n",
       "      <td>0.712447</td>\n",
       "      <td>0.975298</td>\n",
       "      <td>0.682506</td>\n",
       "      <td>0.505597</td>\n",
       "      <td>0.671064</td>\n",
       "      <td>0.328343</td>\n",
       "      <td>0.325759</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.670787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908101</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.561902</td>\n",
       "      <td>0.418131</td>\n",
       "      <td>0.589112</td>\n",
       "      <td>0.480530</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>0.167294</td>\n",
       "      <td>0.195227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.216125</td>\n",
       "      <td>0.886937</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.593890</td>\n",
       "      <td>0.746499</td>\n",
       "      <td>0.232675</td>\n",
       "      <td>0.532753</td>\n",
       "      <td>0.253672</td>\n",
       "      <td>0.722430</td>\n",
       "      <td>0.286589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434542</td>\n",
       "      <td>0.455196</td>\n",
       "      <td>0.393741</td>\n",
       "      <td>0.345357</td>\n",
       "      <td>0.513962</td>\n",
       "      <td>0.626629</td>\n",
       "      <td>0.375813</td>\n",
       "      <td>0.464836</td>\n",
       "      <td>0.484253</td>\n",
       "      <td>0.602888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.601771</td>\n",
       "      <td>0.431577</td>\n",
       "      <td>0.191582</td>\n",
       "      <td>0.579174</td>\n",
       "      <td>0.703823</td>\n",
       "      <td>0.756626</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.083622</td>\n",
       "      <td>0.721005</td>\n",
       "      <td>0.460862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396332</td>\n",
       "      <td>0.047820</td>\n",
       "      <td>0.401636</td>\n",
       "      <td>0.325989</td>\n",
       "      <td>0.569875</td>\n",
       "      <td>0.253208</td>\n",
       "      <td>0.585392</td>\n",
       "      <td>0.732821</td>\n",
       "      <td>0.729521</td>\n",
       "      <td>0.307855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>0.408145</td>\n",
       "      <td>0.284460</td>\n",
       "      <td>0.734342</td>\n",
       "      <td>0.973453</td>\n",
       "      <td>0.697274</td>\n",
       "      <td>0.964572</td>\n",
       "      <td>0.447273</td>\n",
       "      <td>0.110041</td>\n",
       "      <td>0.907507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521647</td>\n",
       "      <td>0.167341</td>\n",
       "      <td>0.499947</td>\n",
       "      <td>0.788225</td>\n",
       "      <td>0.245464</td>\n",
       "      <td>0.413755</td>\n",
       "      <td>0.068488</td>\n",
       "      <td>0.542442</td>\n",
       "      <td>0.714398</td>\n",
       "      <td>0.941287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.927145  0.556246  0.955109  0.995895  0.580252  0.012653  0.367849   \n",
       "1   0.785615  0.903626  0.161089  0.109162  0.710021  0.970078  0.934077   \n",
       "2   0.731707  0.699413  0.831009  0.930310  0.731158  0.069717  0.735959   \n",
       "3   0.992364  0.653169  0.030656  0.280821  0.751770  0.410553  0.246366   \n",
       "4   0.698501  0.435902  0.843922  0.259721  0.797488  0.972315  0.269579   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.202200  0.778028  0.701222  0.303444  0.113863  0.616086  0.370898   \n",
       "96  0.124268  0.712447  0.975298  0.682506  0.505597  0.671064  0.328343   \n",
       "97  0.216125  0.886937  0.100456  0.593890  0.746499  0.232675  0.532753   \n",
       "98  0.601771  0.431577  0.191582  0.579174  0.703823  0.756626  0.055519   \n",
       "99  0.006514  0.408145  0.284460  0.734342  0.973453  0.697274  0.964572   \n",
       "\n",
       "         7         8         9    ...       490       491       492       493  \\\n",
       "0   0.102461  0.875389  0.552692  ...  0.345356  0.451318  0.610774  0.936466   \n",
       "1   0.984116  0.092469  0.922313  ...  0.084911  0.556243  0.226116  0.090481   \n",
       "2   0.918918  0.773546  0.170292  ...  0.840504  0.806163  0.932701  0.988374   \n",
       "3   0.652241  0.486327  0.347260  ...  0.764963  0.698155  0.616626  0.011435   \n",
       "4   0.798582  0.500263  0.250349  ...  0.674325  0.473022  0.449506  0.485336   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.141494  0.655690  0.268150  ...  0.185427  0.651085  0.273585  0.269433   \n",
       "96  0.325759  0.046161  0.670787  ...  0.908101  0.012145  0.055363  0.561902   \n",
       "97  0.253672  0.722430  0.286589  ...  0.434542  0.455196  0.393741  0.345357   \n",
       "98  0.083622  0.721005  0.460862  ...  0.396332  0.047820  0.401636  0.325989   \n",
       "99  0.447273  0.110041  0.907507  ...  0.521647  0.167341  0.499947  0.788225   \n",
       "\n",
       "         494       495       496       497       498       499  \n",
       "0   0.272332  0.052199  0.633744  0.020181  0.474952  0.728678  \n",
       "1   0.392536  0.821097  0.255073  0.535103  0.523670  0.785093  \n",
       "2   0.341064  0.262697  0.091428  0.876601  0.806742  0.863439  \n",
       "3   0.835469  0.283578  0.389541  0.925172  0.602922  0.701857  \n",
       "4   0.349590  0.536662  0.849283  0.123237  0.666335  0.238990  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.933416  0.652839  0.143622  0.074453  0.624741  0.124699  \n",
       "96  0.418131  0.589112  0.480530  0.530452  0.167294  0.195227  \n",
       "97  0.513962  0.626629  0.375813  0.464836  0.484253  0.602888  \n",
       "98  0.569875  0.253208  0.585392  0.732821  0.729521  0.307855  \n",
       "99  0.245464  0.413755  0.068488  0.542442  0.714398  0.941287  \n",
       "\n",
       "[100 rows x 500 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_most_similar(input_word, all_words_embedding, n = 10):\n",
    "    #initialize return dataframe\n",
    "    most_similar = pd.DataFrame(np.zeros(input_word.shape[0], n))\n",
    "    similarity_dist = pd.DataFrame(np.ones(shape = (1,n))) * np.inf\n",
    "    for (col_name, word) in fake_embedding.iteritems():\n",
    "        dist = input_word.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  inf  inf\n",
       "1  inf  inf"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  inf  inf\n",
       "1  inf  inf"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.ones(shape = (2,2)))  * np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion2020",
   "language": "python",
   "name": "propulsion2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
