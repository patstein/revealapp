{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrINj5XRYAPX"
   },
   "source": [
    "Deploy instructions:\n",
    "1. Uncomment every part marked with #ACTIVATE for Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f133c0df11a7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f133c0df11a7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2: setup environment\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "1: outline\n",
    "2: setup environment\n",
    "3: helper functions (modules)\n",
    "3: process text, fit model\n",
    "4: suggest tags\n",
    "5: learn and suggest tags\n",
    "6: function that can call 3, 4 and 5 (the main-to-be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "m0uUOYfCQ3PI",
    "outputId": "bfc95058-f3f0-4273-fae8-db440af9e335"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "4DCpBDIhRQMf",
    "outputId": "92f6bd68-8727-46b7-c86a-d9932e9295ef"
   },
   "outputs": [],
   "source": [
    "project_dir = \"/gdrive/My Drive/Notebooks\"\n",
    "%cd {project_dir} \n",
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMq3-CvyYAPa"
   },
   "source": [
    "# 1. OUTLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANKL9d1NYAPc"
   },
   "outputs": [],
   "source": [
    "### THIS IS NO LONGER ACTUAL ###\n",
    "\n",
    "# s e t u p \n",
    "\n",
    "### LOAD ENV\n",
    "### LOAD PRETRAINED MODEL (BERT embeddings) (maybe only in process text ?)\n",
    "### INITIATE DATA VARIABLES (provisory)\n",
    "\n",
    "\n",
    "# p r o c e s s #\n",
    "\n",
    "### GET: TEXT RAW (json), GET MODEL (where?)    \n",
    "### RETURN: TEXT (SENTENCES, EMBEDDINGS, IDS)\n",
    "\n",
    "\n",
    "# f i n d #\n",
    "\n",
    "### GET: TEXT (SENTENCES, EMBEDDINGS , IDS); GET: TARGET (id/?); GET ACT LEARN MODEL (where?)\n",
    "### RETURN: LABELING (SENTENCES IDS, PROXIMITY SCORE, LABEL (Y/N))\n",
    "\n",
    "\n",
    "# l e a r n   a n d   f i n d #\n",
    "\n",
    "### GET: TEXT (SENTENCES, EMBEDDINGS , IDS); GET: TARGET(ID/?); GET ACT LEARN MODEL (where?); GET: LABELING()\n",
    "### RETURN: ACT LEARN MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhQTzcXQYAPv"
   },
   "source": [
    "# 2. SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-JM1H30YKDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /home/sevi/anaconda3/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already up-to-date: transformers in /home/sevi/anaconda3/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/sevi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/sevi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/sevi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.39 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/sevi/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(\"bad handshake: SysCallError(104, 'ECONNRESET')\"))': /simple/sentence-transformers/\u001b[0m\n",
      "Requirement already up-to-date: sentence-transformers in /home/sevi/anaconda3/lib/python3.7/site-packages (0.2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=2.8.0 in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/sevi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (1.12.39)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (2020.4.4)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/sevi/anaconda3/lib/python3.7/site-packages (from transformers>=2.8.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/sevi/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/sevi/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers>=2.8.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers>=2.8.0->sentence-transformers) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.39 in /home/sevi/anaconda3/lib/python3.7/site-packages (from boto3->transformers>=2.8.0->sentence-transformers) (1.15.39)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/sevi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/sevi/anaconda3/lib/python3.7/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/sevi/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers>=2.8.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/sevi/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers>=2.8.0->sentence-transformers) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# this is handled in the env\n",
    "!pip install -U torch\n",
    "!pip install -U transformers\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMoaTFFRYAPx"
   },
   "outputs": [],
   "source": [
    "# https://www.aclweb.org/anthology/D19-1410.pdf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import scipy\n",
    "import csv\n",
    "import gzip\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import NonUniformImage\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer #this is the same algorithm that's used for sent_tokenize but just offers more methods.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import STSDataReader, LabelSentenceReader, InputExample\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1Db_v8WYAPn"
   },
   "outputs": [],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tciZZTocYAQB"
   },
   "outputs": [],
   "source": [
    "MODEL_PRETRAINED = 'bert-base-nli-mean-tokens'\n",
    "MODEL_ACTUALISED_0 = None\n",
    "\n",
    "SERVER_USER_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Qb6XvPTlYAQH",
    "outputId": "b853c490-1ee8-42b3-9208-55d3f8a74a6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sevi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "nltk.download('punkt') \n",
    "# STOP_WORDS = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fGKLcVx36nj"
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/home/sevi/revealStorage/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b5f3ff4bc542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#set ownership of folder (and all subfolders) to root account\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmomo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/home/sevi/revealStorage/'"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "BlockID = ''\n",
    "path_base = '/home/sevi/revealStorage/' \n",
    "\n",
    "# path_model_pretrained = path_base + '../models/pretrained/' #+ MODEL_PRETRAINED\n",
    "# path_model_adapted = path_base + '../models/adapted/model' #+ str(BlockID)\n",
    "\n",
    "# path_documents = path_base + '../data/raw/documents' #+ str(BlockID)\n",
    "# path_sentences = path_base + '../data/sentences/sentences' #+ str(BlockID)\n",
    "# path_embeddings = path_base + '../data/embeddings/embeddings' #+ str(BlockID)\n",
    "\n",
    "# path_tags = path_base + '../data/tags_user/tags_user' #+ str(BlockID)\n",
    "# path_tags_suggested = path_base + '../data/tags_suggested/tags_suggested' #+ str(BlockID)\n",
    "\n",
    "path_model_pretrained = path_base + 'models/pretrained/' #+ MODEL_PRETRAINED\n",
    "path_model_adapted = path_base + 'models/adapted/model' #+ str(BlockID)\n",
    "\n",
    "path_documents = path_base + 'data/raw/documents' #+ str(BlockID)\n",
    "path_sentences = path_base + 'data/sentences/sentences' #+ str(BlockID)\n",
    "path_embeddings = path_base + 'data/embeddings/embeddings' #+ str(BlockID)\n",
    "\n",
    "path_tags = path_base + 'data/tags_user/tags_user' #+ str(BlockID)\n",
    "path_tags_suggested = path_base + 'data/tags_suggested/tags_suggested' #+ str(BlockID)\n",
    "\n",
    "folders_to_be_created = [\n",
    "      path_model_pretrained\n",
    "    , path_model_adapted[:-(path_model_adapted[::-1].find('/'))]\n",
    "    , path_documents[:-(path_documents[::-1].find('/'))]\n",
    "    , path_sentences[:-(path_sentences[::-1].find('/'))]\n",
    "    , path_embeddings[:-(path_embeddings[::-1].find('/'))]\n",
    "    , path_tags[:-(path_tags[::-1].find('/'))]\n",
    "    , path_tags_suggested[:-(path_tags_suggested[::-1].find('/'))]\n",
    "]\n",
    "\n",
    "#ACTIVATE for Server\n",
    "#create folder structure if not exists:\n",
    "# for directory in folders_to_be_created:\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "# os.chown(path_base, SERVER_USER_ID, SERVER_USER_ID)\n",
    "# for root, dirs, files in os.walk(path_base):   #set ownership of folder (and all subfolders) to root account\n",
    "#     for momo in dirs:  \n",
    "#         os.chown(os.path.join(root, momo), SERVER_USER_ID, SERVER_USER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdIKpMUqYAQO"
   },
   "outputs": [],
   "source": [
    "DummyTextBlock = {\n",
    "    \"DocID\":{\"0\":1,\"1\":2,\"2\":3},\n",
    "    \"FullText\":{\n",
    "        \"0\":\"This is a random document with ID 1. In this document we are talking about cats. And there is also something about mice.\",\n",
    "        \"1\":\"This is a random document with ID 2. In this document we are talking about dogs. And there is also something about mice.\",\n",
    "        \"2\":\"This is a random document with ID 3. this doc is exclusivly about Elephants. So no mice, cats nor anything else.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nosiNOJYAQV"
   },
   "outputs": [],
   "source": [
    "DummyUserTags = {\n",
    "    \"ID\":{\"0\":1,\"1\":2},\n",
    "    \"DocID\":{\"0\":1,\"1\":1},\n",
    "    \"startIdx\":{\"0\":37,\"1\":81},\n",
    "    \"Length\":{\"0\":43,\"1\":39}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pg1g5bWSYAQa"
   },
   "outputs": [],
   "source": [
    "DummyTagSuggestions = {\n",
    "    \"ID\":{\"0\":1,\"1\":2,\"2\":3,\"3\":4},\n",
    "    \"DocID\":{\"0\":1,\"1\":2,\"2\":3,\"3\":3},\n",
    "    \"startIdx\":{\"0\":81,\"1\":81,\"2\":37,\"3\":77},\n",
    "    \"Length\":{\"0\":39,\"1\":39,\"2\":39,\"3\":34},\n",
    "    \"TagID\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1},\n",
    "    \"SimilarityScore\":{\"0\":0.74,\"1\":0.5,\"2\":0.8,\"3\":0.77},\n",
    "    \"Accepted\":{\"0\":None,\"1\":0.0,\"2\":1.0,\"3\":1.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVPZgHnIYAQg"
   },
   "outputs": [],
   "source": [
    "# dummy input\n",
    "df_TEXT = pd.DataFrame(DummyTextBlock)\n",
    "df_TAGS = pd.DataFrame(DummyUserTags)\n",
    "df_SUGGESTIONS = pd.DataFrame(DummyTagSuggestions)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_52ai5zYAQm"
   },
   "outputs": [],
   "source": [
    "# dummy variables to write outputs (en vez de save) (no this is not how it works uuups)\n",
    "\n",
    "DF_TEXT = pd.DataFrame()\n",
    "DF_SENTENCES = pd.DataFrame()\n",
    "DF_ENBEDDINGS = pd.DataFrame()\n",
    "DF_TAGS = pd.DataFrame()\n",
    "DF_SUGGESTIONS = pd.DataFrame()\n",
    "MODEL_ACTUALISED_0 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_GJhS25S5no"
   },
   "source": [
    "#  3. HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qls1LScpYAQ0"
   },
   "outputs": [],
   "source": [
    "# converts a raw df row w. id into a sentence dataframe that is compatible w. tags table\n",
    "def split_in_sentences(DocRow):\n",
    "    \n",
    "    PunktTokenizer = PunktSentenceTokenizer().span_tokenize(DocRow['FullText'])\n",
    "    sent_intervals = [interval for interval in PunktTokenizer]    #returns start and end index for each sentence\n",
    "    text_splitted = [DocRow['FullText'][index_tuple[0]:index_tuple[1]] for index_tuple in sent_intervals]\n",
    "    \n",
    "    starts  = [interval[0] for interval in sent_intervals]\n",
    "    ends    = [interval[1] for interval in sent_intervals]\n",
    "    lengths = [end - end_before for (end, end_before) in zip(ends[1:] , ends[:-1])]\n",
    "    lengths.insert(0,ends[0])\n",
    "    \n",
    "    for idx, length in enumerate(lengths[:-1]):\n",
    "        starts[idx+1] = starts[idx] + length \n",
    "            \n",
    "    df_Sentences = pd.DataFrame({\n",
    "                             'DocID': DocRow['DocID'],\n",
    "                             'SentID': range(len(text_splitted)),\n",
    "                             'SentText': text_splitted,\n",
    "                             'SentStartIdx': starts,\n",
    "                             'SentLength': lengths                            \n",
    "                            })    \n",
    "    \n",
    "    return df_Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5-_5bCSYAQ5"
   },
   "outputs": [],
   "source": [
    "# PROBABLY DELETE THIS\n",
    "\n",
    "# this would adapt (fit/train) the enbeddings model on the text but we don't use this\n",
    "def get_adapted_model(corpus, BlockID, AdaptModel = False):\n",
    "    \n",
    "    model = SentenceTransformer(MODEL_PRETRAINED) # or pretrained+trained, if available?    \n",
    "    # currently, we don't do anything like this, we just use the pretrained model, but:\n",
    "    if AdaptModel:\n",
    "        model = model.fit(corpus)\n",
    "        path_out = '../models/fitted/bert-base-nli-mean-tokens' + '-' + str(BlockID)\n",
    "        model.save(path_out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn9DyySqYARG"
   },
   "outputs": [],
   "source": [
    "# get the corresponding text to a tag row in the UserTags / SugestedTags data (coming from FS backend)\n",
    "def get_tag_sentence (TagRow, Sentences):\n",
    "    SentenceRow = Sentences[\n",
    "        (Sentences['DocID'] == TagRow['DocID']) & \n",
    "        (Sentences['SentStartIdx'] <= TagRow['TagStartIdx']) &\n",
    "        (Sentences['SentStartIdx'] + Sentences['SentLength'] > TagRow['TagStartIdx'])\n",
    "    ]\n",
    "\n",
    "    return SentenceRow['SentText'].values[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHfJU4_UYARK"
   },
   "outputs": [],
   "source": [
    "# get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F0LrzrWYARR"
   },
   "outputs": [],
   "source": [
    "# get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDKPLYwDYARc"
   },
   "outputs": [],
   "source": [
    "# search for similar (dfSentences, tag, model) - no longer needed if active learning is up\n",
    "def suggest_most_similar_cosine(TagRow,sentences_embeddings, df_UserTags, df_Sentences, model, top_n):\n",
    "    TagEmbedding = pd.DataFrame(model.encode([TagRow['TagText']]))\n",
    "    suggestions = df_Sentences\n",
    "    # x = cosine_similarity(TagRow['TagEmbedding'], sentences_embeddings)\n",
    "    \n",
    "    suggestions['SimilarityScore'] = cosine_similarity(TagEmbedding, sentences_embeddings).T\n",
    "    suggestions['TagID'] = TagRow['TagID']\n",
    "    suggestions['TagAccepted'] = None\n",
    "    suggestions = suggestions.sort_values(by = 'SimilarityScore', ascending = False).iloc[0:top_n,:]\n",
    "    suggestions['TagAccepted'].iloc[0] = 1.0\n",
    "    \n",
    "    return(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlUBTwW9YARg"
   },
   "outputs": [],
   "source": [
    "# search for similar (active learning model)\n",
    "def suggest_most_similar(Tag, Sentences, model, topn = 10):\n",
    "    predictions = [model.eval(sentence, Tag) for sentence in Sentences['SentText']]\n",
    "    df_Suggestions = df_Sentences.append(['predictions'], axis = 0)\n",
    "\n",
    "    return df_Suggestions.sort_values(by = 'predictions', ascending = False).head(n = topn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Q-I7T9icL4w"
   },
   "outputs": [],
   "source": [
    "def get_training_pairs(RowSuggestion, df_Suggestions, df_UserTags):\n",
    "    Suggestions = df_Suggestions[\n",
    "                    (df_Suggestions['TagID'] == RowSuggestion['TagID']) & \n",
    "                    (df_Suggestions['SuggestionID'] != RowSuggestion['SuggestionID'])]\n",
    "    # Taged = df_UserTags[df_UserTags['TagId']] == RowSuggestion['TagID'] # it IS already in the suggestions table... but if it won't be: append this to the Suggestions.\n",
    "\n",
    "    Suggestions['SentCompareText'] = RowSuggestion['SentText']\n",
    "    TrainPairs = Suggestions.loc[:,['Accepted','SentText', 'SentCompareText']].dropna()  \n",
    "\n",
    "    return TrainPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRdfrifX8KnY"
   },
   "outputs": [],
   "source": [
    "# this reader works for dataframes.\n",
    "class DFReader:\n",
    "    \"\"\"\n",
    "    Reads our data. Each line contains two sentences (s1_col_idx, s2_col_idx) and one label (score_col_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, s1_col_idx=1, s2_col_idx=2, score_col_idx=0):\n",
    "\n",
    "        self.score_col_idx = score_col_idx\n",
    "        self.s1_col_idx = s1_col_idx\n",
    "        self.s2_col_idx = s2_col_idx\n",
    "\n",
    "    def get_examples(self, df, max_examples=0):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        for id, row in df.iterrows():\n",
    "            score = float(row[self.score_col_idx])\n",
    "            s1 = row[self.s1_col_idx]\n",
    "            s2 = row[self.s2_col_idx]\n",
    "            examples.append(InputExample(guid='temp_df'+str(id), texts=[s1, s2], label=score))\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ak58OhUB8Vhc"
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to a DataLoader ready for training\n",
    "def get_train_loader (df, batch, epoch, model):\n",
    "    logging.info(\"Read active learning dataset\")\n",
    "    train_data = SentencesDataset(examples = DFReader().get_examples(df), model = model)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size = batch)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Configure the training. We skip evaluation in this example\n",
    "    warmup_steps = math.ceil(len(train_data)*epoch/batch*0.1) #10% of train data for warm-up\n",
    "    logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "    return train_dataloader, train_loss, warmup_steps\n",
    "\n",
    "def get_evaluator (df, batch, model):\n",
    "    logging.info(\"Read evaluation dataset\")\n",
    "    eval_data = SentencesDataset(examples = DFReader().get_examples(df, max_examples = 10), model = model) # converts to embedding\n",
    "    eval_dataloader = DataLoader(eval_data, shuffle=False, batch_size = batch)\n",
    "    evaluator = EmbeddingSimilarityEvaluator(eval_dataloader) \n",
    "\n",
    "    return eval_dataloader, evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjvLA4R0li9z"
   },
   "outputs": [],
   "source": [
    "def train_active(df_train, df_eval, model):\n",
    "    train_batch_size = 128 # Where does this belong?\n",
    "    num_epochs = 4 # Where does this belong?\n",
    "\n",
    "    train_dataloader, loss, warmup_steps = get_train_loader(df_train, train_batch_size, num_epochs, model)\n",
    "    eval_dataloader, evaluator = get_evaluator(df_eval, train_batch_size, model)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_objectives=[(train_dataloader, loss)],\n",
    "            evaluator=evaluator,\n",
    "            epochs=num_epochs,\n",
    "            evaluation_steps=100,\n",
    "            warmup_steps=warmup_steps,\n",
    "            #output_path=model_save_path # will use save below in order to overwrite\n",
    "           )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOFh9nTBYAQu"
   },
   "source": [
    "# 4. PROCESS TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQAn_T46YARA"
   },
   "outputs": [],
   "source": [
    "def process_documents_raw(DocumentsRaw, BlockID = 0, AdaptModel = False):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Tthis function: \n",
    "        generates a sentences dataframe from the raw documents\n",
    "        fits the pretrained model on the documents - NO\n",
    "        saves the model with the BlockID (it can be trained further on new documents later)\n",
    "        generates embeddings for all sentences - NO\n",
    "    Returns:\n",
    "        a df of sentences w. sentence ids (processed text)?       \n",
    "    Arguments: \n",
    "        BlockID: the id of the block in the backand database (from json) - to identify the saved model\n",
    "        DocumentsRaw: a dict of raw text strings and their IDs (from json) \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # convert the json dict into a dataframe:\n",
    "    df_DocumentsRaw = pd.DataFrame(\n",
    "        data = DocumentsRaw, \n",
    "        columns = ['DocID','FullText'])     \n",
    "    \n",
    "    \n",
    "    # generate the sentences dataframe:\n",
    "    df_Sentences = pd.concat(list(\n",
    "        df_DocumentsRaw.apply(split_in_sentences, axis = 1)))  \n",
    "\n",
    "\n",
    "    corpus = list(df_Sentences['SentText'])    \n",
    "\n",
    "    # get the pretrained model (to make a copy of it for further training or eval)\n",
    "    # TODO: condition that no actualised model exists\n",
    "    # otherwise that will get overwritten every time docs get uploaded   \n",
    "    model = SentenceTransformer(MODEL_PRETRAINED)   \n",
    "    \n",
    "    \n",
    "    # get adapted model (adapt if required - NOT)             \n",
    "    # model = get_model(\n",
    "        #corpus, BlockID, AdaptModel)\n",
    "\n",
    "    \n",
    "    # create embeddings\n",
    "    df_Embeddings = pd.DataFrame(model.encode(corpus))    \n",
    "    df_Embeddings.index = df_Sentences.index.values \n",
    "\n",
    "#ACTIVATE for Server\n",
    "    # save data for further training and evaluation\n",
    "    df_DocumentsRaw.to_csv(path_documents + str(BlockID) + '.csv')\n",
    "    df_Sentences.to_csv(path_sentences + str(BlockID) + '.csv')\n",
    "    df_Embeddings.to_csv(path_embeddings + str(BlockID) + '.csv')\n",
    "\n",
    "    # save model for further training and evaluation\n",
    "    model.save(path_model_adapted + str(BlockID))  \n",
    "\n",
    "\n",
    "    # TODO: add new documents to the existing data and dont' overwrite it\n",
    "    # (load them, append new, save again)\n",
    "\n",
    "\n",
    "    return df_DocumentsRaw, df_Sentences, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID50qxRAYARF"
   },
   "source": [
    "# 5. SUGGEST SIMILARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zv_RXcE0YARk"
   },
   "outputs": [],
   "source": [
    "def suggest_similars(UserTags, BlockID=0, top_n = 10):\n",
    "    \n",
    "    '''\n",
    "    What this function does: \n",
    "        for each user tag, it finds the closest sentences in the already processed documents, based on a (pre)trained model\n",
    "    Returns:\n",
    "        a dict of tag suggestions and suggestion strength        \n",
    "    Arguments: \n",
    "        UserTags: a Tags table whith reviewed earlier suggestions (json from FS backend)\n",
    "        BlockID: to find the ritht saved model and text data.  \n",
    "        top_n: the number of desired suggestions            \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # convert the json dict into a dataframe:\n",
    "    df_UserTags = pd.DataFrame(UserTags)\n",
    "    df_UserTags.columns = 'TagID', 'DocID', 'TagStartIdx', 'TagLength' \n",
    "    \n",
    "    \n",
    "    # fetch model (already trained or just a copy of the pretrained, depending on history)\n",
    "    model = SentenceTransformer(path_model_adapted + str(BlockID))\n",
    "    # model = model.load(path_model_adapted + str(BlockID))   \n",
    "    model.eval() # is it neccessary to put it into eval mode?\n",
    "    \n",
    "    \n",
    "    df_Sentences = pd.read_csv(path_sentences + str(BlockID) + '.csv')\n",
    "    # df_DocumentsRaw = pd.read_csv(path_documents + str(BlockID)) # probably not neccessary\n",
    "    \n",
    "    \n",
    "    # find the sentences corresponding the tags\n",
    "    df_UserTags['TagText'] = df_UserTags.apply(get_tag_sentence, args = [df_Sentences], axis = 1)\n",
    "    \n",
    "\n",
    "    # create sentence and tag corpuses\n",
    "    corpus_sentences = list(df_Sentences['SentText'])\n",
    "    corpus_tags = list(df_UserTags['TagText'])\n",
    "\n",
    "\n",
    "    # create embeddings - not used?\n",
    "    df_EmbeddingsSent = pd.DataFrame(model.encode(corpus_sentences))    \n",
    "    df_EmbeddingsSent.index = df_Sentences.index.values    \n",
    "    \n",
    "\n",
    "\n",
    "    # generate embeddings for tags to 'compare' them\n",
    "    df_EmbeddingsTags = pd.DataFrame(model.encode(corpus_tags))\n",
    "    df_EmbeddingsTags.index = df_UserTags.index.values    \n",
    "    \n",
    "    # df_UserTags['TagEmbedding'] = df_UserTags['TagText'].apply(model.encode)\n",
    "\n",
    "\n",
    "    # generate tag suggestions\n",
    "    df_TagSuggestions = pd.concat(list(\n",
    "        df_UserTags.apply(\n",
    "            suggest_most_similar_cosine, \n",
    "            args = [df_EmbeddingsSent, \n",
    "                    df_UserTags, \n",
    "                    df_Sentences, \n",
    "                    model, \n",
    "                    top_n], \n",
    "            axis = 1)))\n",
    "    \n",
    "#ACTIVATE for Server    \n",
    "    # save data for further training and evaluation    \n",
    "    df_UserTags.to_csv(path_tags + str(BlockID) + '.csv')\n",
    "    df_TagSuggestions.to_csv(path_tags_suggested + str(BlockID) + '.csv')\n",
    "    \n",
    "    # TODO: change scoring to siamese  \n",
    "\n",
    "    return df_TagSuggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnviG6HBYARo"
   },
   "source": [
    "# 6. LEARN FROM TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eA_P7MaFYARp"
   },
   "outputs": [],
   "source": [
    "def learn_suggest_similars(TagSuggestions, BlockID=0, top_n = 10 ):\n",
    "    \n",
    "    '''\n",
    "    What this function does:\n",
    "        actualises (trains) the existing pretrained model, based on user-generated tags and tag evaluations\n",
    "        finds the sentences closest to the tagged one in a list, based on the actualised model\n",
    "        fits a pretrained model on the sentences of a text string\n",
    "        saves a model fitted on the text   \n",
    "    Returns:\n",
    "        a dict of tag suggestions and suggestion strength\n",
    "    \n",
    "    Arguments: \n",
    "        text_id: the id of the raw text in the backand database (from json)\n",
    "        raw_text: a raw text string (from json)\n",
    "        path_model_pretrained: where the original pretrained model is saved.     \n",
    "    '''\n",
    "    \n",
    "    # convert the json dict into a dataframe:\n",
    "    df_TagSuggestions = pd.DataFrame(TagSuggestions)\n",
    "    df_TagSuggestions.columns = 'SuggestionID', 'DocID', 'TagStartIdx', 'TagLength', 'TagID', 'SimilarityScore', 'Accepted'\n",
    "    \n",
    "        \n",
    "    # fetch model (already trained or just a copy of the pretrained, depending on history)\n",
    "    model = SentenceTransformer(path_model_adapted + str(BlockID))\n",
    "    # model = model.load(path_model_adapted + str(BlockID))   \n",
    "    model.eval() # is it neccessary to put it into eval mode?\n",
    "    \n",
    "    \n",
    "    df_Sentences = pd.read_csv(path_sentences + str(BlockID) + '.csv')    \n",
    "    df_UserTags = pd.read_csv(path_tags + str(BlockID) + '.csv')\n",
    "    # df_DocumentsRaw = pd.read_csv(path_documents + str(BlockID)) # probably not neccessary    \n",
    "    \n",
    "    \n",
    "    # find the sentences corresponding the tags\n",
    "    df_TagSuggestions['SentText'] = df_TagSuggestions.apply(\n",
    "        get_tag_sentence, \n",
    "        args = [df_Sentences], \n",
    "        axis = 1)\n",
    "    \n",
    "    df_UserTags['TagText'] = df_UserTags.apply(\n",
    "        get_tag_sentence, \n",
    "        args = [df_Sentences], \n",
    "        axis = 1)\n",
    "\n",
    "\n",
    "    # create sentence and tag corpuses\n",
    "    corpus_sentences = list(df_Sentences['SentText'])\n",
    "    corpus_suggestions = list(df_TagSuggestions['SentText'])\n",
    "    corpus_tags = list(df_UserTags['TagText'])\n",
    "    \n",
    "\n",
    "    # create embeddings\n",
    "    df_EmbeddingsSent = pd.DataFrame(model.encode(corpus_sentences))    \n",
    "    df_EmbeddingsSent.index = df_Sentences.index.values  \n",
    "\n",
    "    df_EmbeddingsSug = pd.DataFrame(model.encode(corpus_suggestions))    \n",
    "    df_EmbeddingsSug.index = df_TagSuggestions.index.values       \n",
    "\n",
    "    df_EmbeddingsTags = pd.DataFrame(model.encode(corpus_tags))\n",
    "    df_EmbeddingsTags.index = df_UserTags.index.values    \n",
    "    \n",
    "\n",
    "    # create train dababase from suggestion feedback\n",
    "    df_LearnActive = pd.concat(list(\n",
    "        df_TagSuggestions.apply(\n",
    "          get_training_pairs, \n",
    "          args = [df_TagSuggestions, df_UserTags], \n",
    "          axis = 1)))\n",
    "\n",
    "\n",
    "    # train model on the ActiveLearning dataset\n",
    "    model_trained = train_active(df_LearnActive, df_LearnActive, model)\n",
    "\n",
    "    \n",
    "    df_TagsPositive = df_TagSuggestions[df_TagSuggestions['Accepted'] == 1]\n",
    "\n",
    "    print(df_TagsPositive)\n",
    "\n",
    "\n",
    "\n",
    "    # generate tag suggestions (now this just overwrites the recieved one)\n",
    "    # TODO: make it work with just the positive feedback sentences as tags\n",
    "    df_TagSuggestions = pd.concat(list(\n",
    "        df_UserTags.apply(\n",
    "            suggest_most_similar_cosine, \n",
    "            args = [df_EmbeddingsSent, \n",
    "                    df_UserTags, \n",
    "                    df_Sentences, \n",
    "                    model_trained, \n",
    "                    top_n], \n",
    "            axis = 1)))\n",
    "    \n",
    "\n",
    "#ACTIVATE for Server    \n",
    "    # save data for further training and evaluation    \n",
    "    df_TagSuggestions.to_csv(path_tags_suggested + str(BlockID) + '.csv')\n",
    "\n",
    "\n",
    "    # save the trained model for further training and evaluation\n",
    "    model_trained.save(path_model_adapted + str(BlockID))  \n",
    "\n",
    "    \n",
    "    # TODO: change scoring to siamese? \n",
    "    return df_TagSuggestions, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4U1nxtyfYARv"
   },
   "source": [
    "# 7. MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKYPo3trYARw"
   },
   "outputs": [],
   "source": [
    "def do_what_you_are_told (call, args): # something main-like... wait...\n",
    "    \n",
    "    '''\n",
    "    This is just a placeholder / gate function for the api. \n",
    "    It forwards its calls to corresponding functions. These are:\n",
    "    \n",
    "    process_documents_raw:          \n",
    "    suggest_similars:                \n",
    "    learn and suggest_similars: \n",
    "              \n",
    "    '''\n",
    "   \n",
    "    if call == 'process_documents_raw':\n",
    "        answer = process_documents_raw(\n",
    "            args['DocumentsRaw'],\n",
    "            args['BlockID'],\n",
    "            )           \n",
    "        # answer = None\n",
    "\n",
    "        \n",
    "    elif call == 'suggest_similars':                \n",
    "        answer = suggest_similars(                        \n",
    "            args['UserTags'],\n",
    "            args['BlockID'],\n",
    "            args['top_n']\n",
    "            )\n",
    "        # TODO: save answer[0]: Suggestions \n",
    "        # TODO: save answer[1]: Model (if we don't load it in process_docouments_raw)\n",
    "\n",
    "        \n",
    "    elif call == 'learn_suggest_similars':             \n",
    "        answer = learn_suggest_similars(\n",
    "            args['UserTags'],\n",
    "            args['BlockID']\n",
    "            )\n",
    "        # TODO: save answer[0]: Suggestions \n",
    "        # TODO: save answer[1]: ModelTrained\n",
    "\n",
    "           \n",
    "    else: #well... maybe it should throw an error but for now:\n",
    "        answer = print('I wonder why this happened.')\n",
    "                     \n",
    "    return answer       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gifWX67tdrDP"
   },
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LlEvTK03bcJ4",
    "outputId": "b463d09e-e40b-4ed1-ac24-f39a2e158877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 20:30:40 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "2020-04-18 20:30:40 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-04-18 20:30:40 - Load SentenceTransformer from folder: /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "2020-04-18 20:30:40 - loading configuration file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n",
      "2020-04-18 20:30:40 - Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-04-18 20:30:40 - loading weights file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 20:30:43 - Model name '/home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-04-18 20:30:43 - Didn't find file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "2020-04-18 20:30:43 - loading file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n",
      "2020-04-18 20:30:43 - loading file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "2020-04-18 20:30:43 - loading file /home/sevi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "2020-04-18 20:30:43 - loading file None\n",
      "2020-04-18 20:30:43 - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 20:30:43 - Save model to /home/sevi/revealStorage/models/adapted/model0\n",
      "2020-04-18 20:30:43 - Configuration saved in /home/sevi/revealStorage/models/adapted/model0/0_BERT/config.json\n",
      "2020-04-18 20:30:46 - Model weights saved in /home/sevi/revealStorage/models/adapted/model0/0_BERT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "DF_TEXT, DF_SENTENCES, MODEL_ACTUALISED = do_what_you_are_told(\n",
    "    'process_documents_raw',{\n",
    "        'DocumentsRaw': DummyTextBlock, \n",
    "        'BlockID': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3wvRK8B7SDZ2",
    "outputId": "f6fb8955-de4c-4ded-ef6e-63e69d1320eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 20:31:06 - Load pretrained SentenceTransformer: /home/sevi/revealStorage/models/adapted/model0\n",
      "2020-04-18 20:31:06 - Load SentenceTransformer from folder: /home/sevi/revealStorage/models/adapted/model0\n",
      "2020-04-18 20:31:06 - loading configuration file /home/sevi/revealStorage/models/adapted/model0/0_BERT/config.json\n",
      "2020-04-18 20:31:06 - Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-04-18 20:31:06 - loading weights file /home/sevi/revealStorage/models/adapted/model0/0_BERT/pytorch_model.bin\n",
      "2020-04-18 20:31:08 - Model name '/home/sevi/revealStorage/models/adapted/model0/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/home/sevi/revealStorage/models/adapted/model0/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-04-18 20:31:08 - Didn't find file /home/sevi/revealStorage/models/adapted/model0/0_BERT/added_tokens.json. We won't load it.\n",
      "2020-04-18 20:31:08 - loading file /home/sevi/revealStorage/models/adapted/model0/0_BERT/vocab.txt\n",
      "2020-04-18 20:31:08 - loading file None\n",
      "2020-04-18 20:31:08 - loading file /home/sevi/revealStorage/models/adapted/model0/0_BERT/special_tokens_map.json\n",
      "2020-04-18 20:31:08 - loading file /home/sevi/revealStorage/models/adapted/model0/0_BERT/tokenizer_config.json\n",
      "2020-04-18 20:31:08 - Use pytorch device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /home/sevi/revealStorage/data/sentences/sentences0.csv does not exist: '/home/sevi/revealStorage/data/sentences/sentences0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a4df37fde391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'UserTags'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDummyUserTags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'BlockID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m'top_n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         })\n",
      "\u001b[0;32m<ipython-input-28-5606c0fbc2b4>\u001b[0m in \u001b[0;36mdo_what_you_are_told\u001b[0;34m(call, args)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UserTags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BlockID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top_n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             )\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# TODO: save answer[0]: Suggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-500c30975198>\u001b[0m in \u001b[0;36msuggest_similars\u001b[0;34m(UserTags, BlockID, top_n)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf_Sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_sentences\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBlockID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# df_DocumentsRaw = pd.read_csv(path_documents + str(BlockID)) # probably not neccessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /home/sevi/revealStorage/data/sentences/sentences0.csv does not exist: '/home/sevi/revealStorage/data/sentences/sentences0.csv'"
     ]
    }
   ],
   "source": [
    "DF_SUGGESTIONS = do_what_you_are_told(\n",
    "    'suggest_similars',{\n",
    "        'UserTags': DummyUserTags, \n",
    "        'BlockID': 0,\n",
    "        'top_n': 10\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "id": "BhF-XNPPw0Jy",
    "outputId": "d8945403-54f4-4f9d-bff3-137ecb8534ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DocID</th>\n",
       "      <th>SentID</th>\n",
       "      <th>SentText</th>\n",
       "      <th>SentStartIdx</th>\n",
       "      <th>SentLength</th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>TagID</th>\n",
       "      <th>TagAccepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about cats.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.612529</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.612529</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 3. this doc ...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.447612</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about dogs.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.396784</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>So no mice, cats nor anything else.</td>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>0.351652</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 2.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.253938</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 1.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.197285</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 1.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 2.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.911030</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 3. this doc ...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.557656</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>So no mice, cats nor anything else.</td>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>0.341533</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about dogs.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.228636</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.225277</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.225277</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about cats.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.197285</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  DocID  SentID  ... SimilarityScore  TagID  TagAccepted\n",
       "1           1      1       1  ...        1.000000      1            1\n",
       "2           2      1       2  ...        0.612529      1         None\n",
       "5           2      2       2  ...        0.612529      1         None\n",
       "6           0      3       0  ...        0.447612      1         None\n",
       "4           1      2       1  ...        0.396784      1         None\n",
       "7           1      3       1  ...        0.351652      1         None\n",
       "3           0      2       0  ...        0.253938      1         None\n",
       "0           0      1       0  ...        0.197285      1         None\n",
       "0           0      1       0  ...        1.000000      2            1\n",
       "3           0      2       0  ...        0.911030      2         None\n",
       "6           0      3       0  ...        0.557656      2         None\n",
       "7           1      3       1  ...        0.341533      2         None\n",
       "4           1      2       1  ...        0.228636      2         None\n",
       "2           2      1       2  ...        0.225277      2         None\n",
       "5           2      2       2  ...        0.225277      2         None\n",
       "1           1      1       1  ...        0.197285      2         None\n",
       "\n",
       "[16 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_SUGGESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3p7_acfOsuqK"
   },
   "outputs": [],
   "source": [
    "DummyTagSuggestionsNew = {\n",
    "    \"ID\":{\"0\":1,\"1\":2,\"2\":3,\"3\":4},\n",
    "    \"DocID\":{\"0\":1,\"1\":2,\"2\":3,\"3\":3},\n",
    "    \"startIdx\":{\"0\":79,\"1\":79,\"2\":0,\"3\":76},\n",
    "    \"Length\":{\"0\":39,\"1\":39,\"2\":39,\"3\":34},\n",
    "    \"TagID\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1},\n",
    "    \"SimilarityScore\":{\"0\":0.74,\"1\":0.5,\"2\":0.8,\"3\":0.77},\n",
    "    \"Accepted\":{\"0\":None,\"1\":0.0,\"2\":1.0,\"3\":1.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "d8SuYxIAybL0",
    "outputId": "da48f069-6cc3-46ce-8136-6fef5a69cf12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DocID</th>\n",
       "      <th>startIdx</th>\n",
       "      <th>Length</th>\n",
       "      <th>TagID</th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DocID  startIdx  Length  TagID  SimilarityScore  Accepted\n",
       "0   1      1        79      39      1             0.74       NaN\n",
       "1   2      2        79      39      1             0.50       0.0\n",
       "2   3      3         0      39      1             0.80       1.0\n",
       "3   4      3        76      34      1             0.77       1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(DummyTagSuggestionsNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KMJp2IXMTYGA",
    "outputId": "1f364bd9-4337-48fb-df60-f5ef7d2e23e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:10:56 - Load pretrained SentenceTransformer: ../models/adapted/model0\n",
      "2020-04-18 11:10:56 - Load SentenceTransformer from folder: ../models/adapted/model0\n",
      "2020-04-18 11:10:56 - loading configuration file ../models/adapted/model0/0_BERT/config.json\n",
      "2020-04-18 11:10:56 - Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-04-18 11:10:56 - loading weights file ../models/adapted/model0/0_BERT/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:10:59 - Model name '../models/adapted/model0/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../models/adapted/model0/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-04-18 11:10:59 - loading file ../models/adapted/model0/0_BERT/vocab.txt\n",
      "2020-04-18 11:10:59 - loading file ../models/adapted/model0/0_BERT/added_tokens.json\n",
      "2020-04-18 11:10:59 - loading file ../models/adapted/model0/0_BERT/special_tokens_map.json\n",
      "2020-04-18 11:10:59 - loading file ../models/adapted/model0/0_BERT/tokenizer_config.json\n",
      "2020-04-18 11:10:59 - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00,  2.50it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00,  7.71it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "Convert dataset: 100%|| 9/9 [00:00<00:00, 2098.67it/s]\n",
      "Convert dataset: 100%|| 9/9 [00:00<00:00, 1495.53it/s]\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:01 - Read active learning dataset\n",
      "2020-04-18 11:11:01 - Num sentences: 9\n",
      "2020-04-18 11:11:01 - Sentences 0 longer than max_seqence_length: 0\n",
      "2020-04-18 11:11:01 - Sentences 1 longer than max_seqence_length: 0\n",
      "2020-04-18 11:11:01 - Warmup-steps: 1\n",
      "2020-04-18 11:11:01 - Read evaluation dataset\n",
      "2020-04-18 11:11:01 - Num sentences: 9\n",
      "2020-04-18 11:11:01 - Sentences 0 longer than max_seqence_length: 0\n",
      "2020-04-18 11:11:01 - Sentences 1 longer than max_seqence_length: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|| 1/1 [00:08<00:00,  8.56s/it]\n",
      "Epoch:   0%|          | 0/4 [00:08<?, ?it/s]\n",
      "Convert Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:09 - Evaluation the model on  dataset after epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert Evaluating: 100%|| 1/1 [00:01<00:00,  1.51s/it]\n",
      "Epoch:  25%|       | 1/4 [00:10<00:30, 10.12s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:11 - Cosine-Similarity :\tPearson: -0.4581\tSpearman: -0.0949\n",
      "2020-04-18 11:11:11 - Manhattan-Distance:\tPearson: -0.4622\tSpearman: -0.0949\n",
      "2020-04-18 11:11:11 - Euclidean-Distance:\tPearson: -0.4689\tSpearman: -0.0949\n",
      "2020-04-18 11:11:11 - Dot-Product-Similarity:\tPearson: -0.4751\tSpearman: -0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|| 1/1 [00:07<00:00,  7.10s/it]\n",
      "Epoch:  25%|       | 1/4 [00:17<00:30, 10.12s/it]\n",
      "Convert Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:18 - Evaluation the model on  dataset after epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert Evaluating: 100%|| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Epoch:  50%|     | 2/4 [00:18<00:19,  9.70s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:20 - Cosine-Similarity :\tPearson: -0.4749\tSpearman: -0.2847\n",
      "2020-04-18 11:11:20 - Manhattan-Distance:\tPearson: -0.4955\tSpearman: -0.2847\n",
      "2020-04-18 11:11:20 - Euclidean-Distance:\tPearson: -0.4988\tSpearman: -0.2847\n",
      "2020-04-18 11:11:20 - Dot-Product-Similarity:\tPearson: -0.4626\tSpearman: -0.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|| 1/1 [00:06<00:00,  6.71s/it]\n",
      "Epoch:  50%|     | 2/4 [00:25<00:19,  9.70s/it]\n",
      "Convert Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:26 - Evaluation the model on  dataset after epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert Evaluating: 100%|| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Epoch:  75%|  | 3/4 [00:27<00:09,  9.29s/it]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:28 - Cosine-Similarity :\tPearson: -0.3241\tSpearman: -0.0949\n",
      "2020-04-18 11:11:28 - Manhattan-Distance:\tPearson: -0.4244\tSpearman: -0.0949\n",
      "2020-04-18 11:11:28 - Euclidean-Distance:\tPearson: -0.4225\tSpearman: -0.0949\n",
      "2020-04-18 11:11:28 - Dot-Product-Similarity:\tPearson: -0.3523\tSpearman: -0.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|| 1/1 [00:06<00:00,  6.67s/it]\n",
      "Epoch:  75%|  | 3/4 [00:33<00:09,  9.29s/it]\n",
      "Convert Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:35 - Evaluation the model on  dataset after epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert Evaluating: 100%|| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Epoch: 100%|| 4/4 [00:35<00:00,  8.86s/it]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:36 - Cosine-Similarity :\tPearson: -0.2296\tSpearman: -0.0949\n",
      "2020-04-18 11:11:36 - Manhattan-Distance:\tPearson: -0.3663\tSpearman: -0.0949\n",
      "2020-04-18 11:11:36 - Euclidean-Distance:\tPearson: -0.3611\tSpearman: -0.0949\n",
      "2020-04-18 11:11:36 - Dot-Product-Similarity:\tPearson: -0.2654\tSpearman: -0.0949\n",
      "   SuggestionID  ...                                           SentText\n",
      "2             3  ...  This is a random document with ID 3. this doc ...\n",
      "3             4  ...                So no mice, cats nor anything else.\n",
      "\n",
      "[2 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 10.55it/s]\n",
      "Batches: 100%|| 1/1 [00:00<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-18 11:11:37 - Save model to ../models/adapted/model0\n",
      "2020-04-18 11:11:37 - Configuration saved in ../models/adapted/model0/0_BERT/config.json\n",
      "2020-04-18 11:11:38 - Model weights saved in ../models/adapted/model0/0_BERT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "DF_TAGS3, MODEL_ACTUALISED = do_what_you_are_told(\n",
    "    'learn_suggest_similars',{\n",
    "        'UserTags': DummyTagSuggestionsNew, \n",
    "        'BlockID': 0,\n",
    "        'top_n': 10\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "id": "N-yxlFXiQVz2",
    "outputId": "4f5b8e48-1e0b-4fcc-b618-17b9088df19e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DocID</th>\n",
       "      <th>SentID</th>\n",
       "      <th>SentText</th>\n",
       "      <th>SentStartIdx</th>\n",
       "      <th>SentLength</th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>TagID</th>\n",
       "      <th>TagAccepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about cats.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.921309</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.645002</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.645002</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 3. this doc ...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.578353</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>So no mice, cats nor anything else.</td>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>0.560182</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 2.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.376421</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 1.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.342225</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about dogs.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.286883</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 1.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.832507</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 2.</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.806075</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>So no mice, cats nor anything else.</td>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>0.683793</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a random document with ID 3. this doc ...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.337166</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>And there is also something about mice.</td>\n",
       "      <td>79</td>\n",
       "      <td>39</td>\n",
       "      <td>0.337166</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about cats.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.266440</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>In this document we are talking about dogs.</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>0.150107</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  DocID  SentID  ... SimilarityScore  TagID  TagAccepted\n",
       "1           1      1       1  ...        0.921309      1            1\n",
       "2           2      1       2  ...        0.645002      1         None\n",
       "5           2      2       2  ...        0.645002      1         None\n",
       "6           0      3       0  ...        0.578353      1         None\n",
       "7           1      3       1  ...        0.560182      1         None\n",
       "3           0      2       0  ...        0.376421      1         None\n",
       "0           0      1       0  ...        0.342225      1         None\n",
       "4           1      2       1  ...        0.286883      1         None\n",
       "0           0      1       0  ...        0.832507      2            1\n",
       "3           0      2       0  ...        0.806075      2         None\n",
       "7           1      3       1  ...        0.683793      2         None\n",
       "6           0      3       0  ...        0.561181      2         None\n",
       "2           2      1       2  ...        0.337166      2         None\n",
       "5           2      2       2  ...        0.337166      2         None\n",
       "1           1      1       1  ...        0.266440      2         None\n",
       "4           1      2       1  ...        0.150107      2         None\n",
       "\n",
       "[16 rows x 9 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_TAGS3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS8SCMHTsLoy"
   },
   "source": [
    "# PLAYGROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1m38fV4sxsh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SOFh9nTBYAQu",
    "ID50qxRAYARF",
    "BnviG6HBYARo",
    "4U1nxtyfYARv"
   ],
   "name": "structure_outline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "propulsion2020",
   "language": "python",
   "name": "propulsion2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
