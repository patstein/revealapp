{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText + NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Author__: Patrick Steiner\n",
    "\n",
    "__Created__: 31.03.2020\n",
    "\n",
    "__Version__: 3\n",
    "\n",
    "__Description__: Creates sentence embedding based on tf-idf and word-embedding and feeds this in a NN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Change ID | Date       |  Author             |Change Description                   |\n",
    "|----------|------------|---------------------|--------------------------------------|\n",
    "|#SK01     | 03.04.2020 | Severin Kappeler    |Re-define corpus with split by sentence rather than by document | \n",
    "|#SK02     | 03.04.2020 | Severin Kappeler    |Embedding matrix weighted by TF-IDF score for each word-embedding vector. Using here the \"global\" TF-IDF. For a given word the \"global\" TF-IDF score is the average of all the TF-IDF scores for that word over all documents.  | \n",
    "|#SK03     | 03.04.2020 | Severin Kappeler    |Apply tokenize.texts_to_sequence to sentence list rather than document list| \n",
    "|#SK04     | 03.04.2020 | Severin Kappeler    |Converting label (target) to be on sentence level rather than document level.      | \n",
    "|#SK05     | 03.04.2020 | Severin Kappeler    |Adding split to sentence part for features |\n",
    "|#SK06     | 03.07.2020 | Katalin Horvath     |Replace Word2vec with FastText, adapt TF-IDF vocabulary to FastText|\n",
    "|#SK07     | 03.07.2020 | Katalin Horvath     |Replace training data with similarity scores data|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see also 06-Natural-Language_processing notebooks. This one focuses only on doc2vec.\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import smart_open\n",
    "import collections\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import tqdm\n",
    "import warnings\n",
    "import io\n",
    "from pprint import pprint as print\n",
    "import fasttext as FT\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "#from gensim.models.wrappers import FastText\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,roc_auc_score, roc_curve, mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.model_selection import GridSearchCV, RandomSearch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just for now, because Windows. A helper with somme missing methods from fasttext. Needed for loadin the .bin pretrained model.\n",
    "# Othervise, just use fasstext.util (imported as FT.util) (also not the gensim one)\n",
    "import FT_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/Users/patrickrs/Documents/GitLab/patrick-steiner/Exercises')\n",
    "news1_raw = pd.read_csv('../data/raw/2012.SMTnews.test.csv', sep='\\t', header=None, names=['SimilarityScore', 'sentence1', 'sentence2'])\n",
    "news2_raw = pd.read_csv('../data/raw/2014.deft-news.test.csv', sep='\\t', header=None, names=['SimilarityScore', 'sentence1', 'sentence2'])\n",
    "\n",
    "news_raw = news1_raw.copy()\n",
    "news_raw = news_raw.append(news2_raw)\n",
    "news_raw.drop_duplicates(inplace=True)\n",
    "news_raw.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark pairs rated under 2.0 SimilarityScore as unsimilar (0) and similar (1) above that:\n",
    "news_raw['binary'] = news_raw['SimilarityScore'].apply(lambda x: 0 if x<2 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.000</td>\n",
       "      <td>Last year he was wanted for murder.</td>\n",
       "      <td>Last year it was sought to murder.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000</td>\n",
       "      <td>Pro-market economists don't object to corporat...</td>\n",
       "      <td>Economists' not against the companies openly u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000</td>\n",
       "      <td>And, perhaps most importantly, Ahmadinejad is ...</td>\n",
       "      <td>And, perhaps the most important, Ahmadinejad d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.667</td>\n",
       "      <td>A Europe for All</td>\n",
       "      <td>All Europe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.500</td>\n",
       "      <td>Gays and other \"modern\" practices are rejected...</td>\n",
       "      <td>The gay and other practical \"Modern\" are rejec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1.800</td>\n",
       "      <td>indian and pakistani governments nearly engage...</td>\n",
       "      <td>indian and pakistani governments each conducte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>3.800</td>\n",
       "      <td>iguaran stated that the detainees will be accu...</td>\n",
       "      <td>iguaran stated that the detainees were also in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2.400</td>\n",
       "      <td>3 suspected extremists were released on bail.</td>\n",
       "      <td>1 suspected extremist was provisionally releas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>0.800</td>\n",
       "      <td>6 czech hospital employees are charged with hu...</td>\n",
       "      <td>the accused will be charged with international...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>2.000</td>\n",
       "      <td>this further aggravated the situation.</td>\n",
       "      <td>north korea should not act further to aggravat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>654 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SimilarityScore                                          sentence1  \\\n",
       "0              4.000                Last year he was wanted for murder.   \n",
       "1              5.000  Pro-market economists don't object to corporat...   \n",
       "2              5.000  And, perhaps most importantly, Ahmadinejad is ...   \n",
       "3              4.667                                   A Europe for All   \n",
       "4              4.500  Gays and other \"modern\" practices are rejected...   \n",
       "..               ...                                                ...   \n",
       "649            1.800  indian and pakistani governments nearly engage...   \n",
       "650            3.800  iguaran stated that the detainees will be accu...   \n",
       "651            2.400     3 suspected extremists were released on bail.    \n",
       "652            0.800  6 czech hospital employees are charged with hu...   \n",
       "653            2.000            this further aggravated the situation.    \n",
       "\n",
       "                                             sentence2  binary  \n",
       "0                 Last year it was sought to murder.         1  \n",
       "1    Economists' not against the companies openly u...       1  \n",
       "2    And, perhaps the most important, Ahmadinejad d...       1  \n",
       "3                                           All Europe       1  \n",
       "4    The gay and other practical \"Modern\" are rejec...       1  \n",
       "..                                                 ...     ...  \n",
       "649  indian and pakistani governments each conducte...       0  \n",
       "650  iguaran stated that the detainees were also in...       1  \n",
       "651  1 suspected extremist was provisionally releas...       1  \n",
       "652  the accused will be charged with international...       0  \n",
       "653  north korea should not act further to aggravat...       1  \n",
       "\n",
       "[654 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, X_train_2, X_test_2 ,y_train, y_test = train_test_split(\n",
    "    news_raw['sentence1'], \n",
    "    news_raw['sentence2'],\n",
    "    news_raw['binary'],\n",
    "    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "131\n",
      "523\n",
      "131\n",
      "523\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_1))\n",
    "print(len(X_test_1))\n",
    "print(len(X_train_2))\n",
    "print(len(X_test_2))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function same as in preprocess notebook\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    text_tokenized = nltk.word_tokenize(text)\n",
    "    text_tokenized = [x.lower() for x in text_tokenized]\n",
    "    wordlist = []\n",
    "    for word in text_tokenized: \n",
    "        \n",
    "        if ((word not in STOP_WORDS) and (len(word)>2)):            \n",
    "            wordlist.append(word)\n",
    "        # if not (word.isalnum()):            \n",
    "        # text_tokenized.remove(word)\n",
    "            \n",
    "    return(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_clean = [tokenize_text(sentence) for sentence in X_train_1]\n",
    "X_train_2_clean = [tokenize_text(sentence) for sentence in X_train_2]\n",
    "\n",
    "X_test_1_clean = [tokenize_text(sentence) for sentence in X_test_1]\n",
    "X_test_2_clean = [tokenize_text(sentence) for sentence in X_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = X_train_1.append(X_train_2)\n",
    "X_train_all.index = range(len(X_train_all))\n",
    "\n",
    "X_test_all = X_test_1.append(X_test_2)\n",
    "X_test_all.index = range(len(X_test_all))\n",
    "\n",
    "X_train_all_clean = X_train_1_clean + X_train_2_clean\n",
    "#X_train_all_clean = X_train_1_clean + X_train_2_clean\n",
    "\n",
    "X_test_all_clean = X_test_1_clean + X_test_2_clean\n",
    "#X_test_all_clean = X_test_1_clean + X_test_2_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText (Pretrained on Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/fasttext.html\n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the pretrained fasttext word embeddings, but no model, not trainable, no word segment embeddings\n",
    "\n",
    "# model_pt = gensim.models.KeyedVectors.load_word2vec_format('../wiki-news-300d-1M.vec') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Use --overwrite to download anyway.\n"
     ]
    }
   ],
   "source": [
    "# only do this once, it is big\n",
    "FT_util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft = FT.load_model('cc.en.300.bin')\n",
    "#ft.save_model('cc.en.300.bin') # just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fasttext.FastText._FastText'>\n",
      "300\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(type(ft))\n",
    "print(ft.get_dimension())\n",
    "print(ft.get_word_vector('hello').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK02\n",
    "#this function creates the (for now only global) TF-IDF weights for each word.\n",
    "def calc_tf_idf(cleaned_doc_str = None, vocab = None):\n",
    "    #Calculate TF-IDF score for all words in the corpus\n",
    "    tfidf = TfidfVectorizer(\n",
    "          sublinear_tf=True\n",
    "        , min_df=1\n",
    "        , norm='l2'\n",
    "        , encoding='latin-1'\n",
    "        , ngram_range=(1,1)  #considering only 1-grams for now\n",
    "        , token_pattern= '(?u)\\\\b\\\\w+\\\\b' #changed from default in order to also include single letters in case they appear in the corpus.\n",
    "        , tokenizer=nltk.word_tokenize #we pass the corpus as words already.\n",
    "        , stop_words=None #we already removed stopwords\n",
    "        , vocabulary = vocab\n",
    "    ) \n",
    "    tfidf_matrix = tfidf.fit_transform(cleaned_doc_str).toarray()    \n",
    "    tfidf_matrix = pd.DataFrame(tfidf_matrix, columns = tfidf.get_feature_names())\n",
    "                                    \n",
    "    #if(not(len(tfidf.get_feature_names()) == len(model_3.wv.vocab.keys()))):\n",
    "    #   warnings.warn('The count of vocabulary from the word embedding does not coincide with the TF-IDF Vectorizer features.')\n",
    "    \n",
    "    return tfidf_matrix\n",
    "    #None\n",
    "    #tfidf_matrix contains TF-IDF scores for each document (in the rows) and word (in the columns)\n",
    "    #tfidf.get_feature_names() gives you the words in the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1046, 2202)\n",
      "(262, 1038)\n"
     ]
    }
   ],
   "source": [
    "X_train_all_tfidf = calc_tf_idf(X_train_all)\n",
    "X_test_all_tfidf = calc_tf_idf(X_test_all)\n",
    "print(X_train_all_tfidf.shape)\n",
    "print(X_test_all_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%</th>\n",
       "      <th>'</th>\n",
       "      <th>''</th>\n",
       "      <th>'are</th>\n",
       "      <th>'malaise</th>\n",
       "      <th>'only</th>\n",
       "      <th>'s</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>...</th>\n",
       "      <th>yugoslavia</th>\n",
       "      <th>zabol</th>\n",
       "      <th>zahedan</th>\n",
       "      <th>zambians</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1046 rows × 2202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        %    '   ''  'are  'malaise  'only        's    (    )         ,  ...  \\\n",
       "0     0.0  0.0  0.0   0.0       0.0    0.0  0.201194  0.0  0.0  0.256965  ...   \n",
       "1     0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "2     0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.135182  ...   \n",
       "3     0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.213887  ...   \n",
       "4     0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "...   ...  ...  ...   ...       ...    ...       ...  ...  ...       ...  ...   \n",
       "1041  0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "1042  0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "1043  0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "1044  0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.000000  ...   \n",
       "1045  0.0  0.0  0.0   0.0       0.0    0.0  0.000000  0.0  0.0  0.306327  ...   \n",
       "\n",
       "      yugoslavia  zabol  zahedan  zambians  zhang  zimbabwe    ‘    ’    “  \\\n",
       "0            0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "1            0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "2            0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "3            0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "4            0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "...          ...    ...      ...       ...    ...       ...  ...  ...  ...   \n",
       "1041         0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "1042         0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "1043         0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "1044         0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "1045         0.0    0.0      0.0       0.0    0.0       0.0  0.0  0.0  0.0   \n",
       "\n",
       "        ”  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "1041  0.0  \n",
       "1042  0.0  \n",
       "1043  0.0  \n",
       "1044  0.0  \n",
       "1045  0.0  \n",
       "\n",
       "[1046 rows x 2202 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine FastText & TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should there be a way to weight tfidf weights...?\n",
    "\n",
    "(means: possibility to play with how mucht influence it has on the resulting embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through tfidf feature_names (the model vocab can be much larger since pretrained. But we are only interested in our own words from the corpus.)\n",
    "def embedding_sentence(sentence, model, tfidf):\n",
    "    '''\n",
    "    This function is to average all words vectors in a given sentence, wegihting them by their tfidf-score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : a list of token strings\n",
    "    \n",
    "    model : a word embedding model \n",
    "            which can give an embedding vector \n",
    "            for each token in sentence\n",
    "            \n",
    "    tfidf : the tfidf weights vector of sentence\n",
    "    '''\n",
    "    sentence_embedding = pd.DataFrame([model.get_word_vector(i)*tfidf[i] for i in sentence]).mean()\n",
    "    return(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_1_embeddings = pd.DataFrame(model.embed_sentences(X_train_1))\n",
    "#X_train_2_embeddings = pd.DataFrame(model.embed_sentences(X_train_2))\n",
    "X_train_all_embeddings = pd.DataFrame([embedding_sentence(X_train_all_clean[i], model_4, X_train_all_tfidf.iloc[i,:]) for i in range(len(X_train_all_clean))])\n",
    "\n",
    "#X_test_1_embeddings = pd.DataFrame(model.embed_sentences(X_test_1))#\n",
    "#X_test_2_embeddings = pd.DataFrame(model.embed_sentences(X_test_2))\n",
    "X_test_all_embeddings = pd.DataFrame([embedding_sentence(X_test_all_clean[i], model_4, X_test_all_tfidf.iloc[i,:]) for i in range(len(X_test_all_clean))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_embeddings = X_train_all_embeddings[:len(X_train_1)]\n",
    "X_train_2_embeddings = X_train_all_embeddings[len(X_train_1):]\n",
    "X_train_2_embeddings.index = X_train_1_embeddings.index\n",
    "\n",
    "X_test_1_embeddings = X_test_all_embeddings[:len(X_test_1)]\n",
    "X_test_2_embeddings = X_test_all_embeddings[len(X_test_1):]\n",
    "X_test_2_embeddings.index = X_test_1_embeddings.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sim = np.array([(\n",
    "    cosine_similarity(\n",
    "        np.array(X_train_1_embeddings.iloc[i,:]).reshape(1,-1),\n",
    "        np.array(X_train_2_embeddings.iloc[i,:]).reshape(1,-1)\n",
    "    ))[0,0] for i in range(len(X_train_1))])\n",
    "\n",
    "X_test_sim = np.array([(\n",
    "    cosine_similarity(\n",
    "        np.array(X_test_1_embeddings.iloc[i,:]).reshape(1,-1),\n",
    "        np.array(X_test_2_embeddings.iloc[i,:]).reshape(1,-1)\n",
    "    ))[0,0] for i in range(len(X_test_1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARY / LOGIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16030534351145037"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(class_weight = 'balanced')\n",
    "clf = logit.fit(X = X_train_sim.reshape(-1, 1), y = y_train)\n",
    "y_pred = clf.predict(X_test_sim.reshape(-1, 1))\n",
    "mse_baseline = mean_squared_error(y_true = y_test, y_pred = y_pred)\n",
    "mse_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    " # generate a no skill prediction (majority class)\n",
    "ns_probs = [1 for _ in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_probs = logit.predict_proba(X_test_sim.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    " # keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No Skill: ROC AUC=0.500'\n",
      "'Logistic: ROC AUC=0.934'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here on, this notebook is not actualised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK03\n",
    "feature_size = 500\n",
    "tokenizer = Tokenizer(num_words = feature_size, split = ' ')\n",
    "# fit the tokenizer on our text\n",
    "tokenizer.fit_on_texts(list(features_all)) #SK03 changed to be applied son sent_clean instead of features (which is grouped by docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the tokens in a matrix\n",
    "X_1 = tokenizer.texts_to_sequences(features_1)\n",
    "X_1 = pad_sequences(X_1)\n",
    "\n",
    "#len(X) = number of sentences in all docsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK04\n",
    "def calc_target_per_sent(sentence_per_doc = None, target_original = None):\n",
    "    \n",
    "    current_target_idx = 0\n",
    "    new_target = pd.DataFrame()\n",
    "    for doc in sentence_per_doc:\n",
    "        for sentence in doc:\n",
    "            new_target = new_target.append(target.iloc[current_target_idx:current_target_idx+1])\n",
    "            \n",
    "        current_target_idx += 1\n",
    "\n",
    "        \n",
    "    return new_target.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = calc_target_per_sent(doc_sent_clean, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the labels\n",
    "y = pd.get_dummies(target).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "model = Sequential()\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(len(global_word_tfidf.keys()) + 1 ,\n",
    "                    300,\n",
    "                    input_length  = X.shape[1],\n",
    "                    weights = [embedding_matrix],\n",
    "                    trainable=False))\n",
    "# learn the correlations\n",
    "model.add(LSTM(300,return_sequences=False))\n",
    "model.add(Dense(12,activation=\"softmax\")) \n",
    "# output model skeleton\n",
    "model.summary()\n",
    "model.compile(optimizer=\"nadam\",loss=\"categorical_crossentropy\",metrics=['acc'],learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca. 15 min / epoch\n",
    "batch = 64\n",
    "epochs = 15\n",
    "model.fit(X_train,y_train,batch,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
