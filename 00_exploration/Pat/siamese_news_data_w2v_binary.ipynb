{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import smart_open\n",
    "import collections\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Nadam, Adagrad, Adamax, Ftrl, RMSprop, SGD #schedules\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM, Conv1D, Conv2D, GlobalAveragePooling1D, Conv2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalAveragePooling2D, GlobalAveragePooling3D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Subtract, Add, Multiply, Activation, Input, Concatenate, Reshape, Dot, GRU \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.model_selection import GridSearchCV, RandomSearch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import svm, tree\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickrs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickrs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " path = '/Users/patrickrs/Documents/GitLab/revealapp/10_cleaning/src'\n",
    "\n",
    "current_path = os.getcwd()\n",
    "os.chdir(path)\n",
    "%run ./Load+Clean_News.ipynb\n",
    "%run ./cont_to_cat_News.ipynb\n",
    "os.chdir(current_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>SimilarityScore</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.000</td>\n",
       "      <td>last year wanted murder</td>\n",
       "      <td>last year sought murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.000</td>\n",
       "      <td>promarket economists dont object corporations blatantly use snob appeal promote products</td>\n",
       "      <td>economists companies openly using attractiveness luxury promote products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5.000</td>\n",
       "      <td>perhaps importantly ahmadinejad destabilizing influence bernanke</td>\n",
       "      <td>perhaps important ahmadinejad destabilising influence bernanke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.667</td>\n",
       "      <td>europe</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4.500</td>\n",
       "      <td>gays modern practices rejected selfindulgent</td>\n",
       "      <td>gay practical modern rejected laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>indian pakistani governments nearly engaged fourth conflict 1999</td>\n",
       "      <td>indian pakistani governments conducted nuclear tests may 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1</td>\n",
       "      <td>3.800</td>\n",
       "      <td>iguaran stated detainees accused homicide criminal collaboration kidnappings funding terrorism</td>\n",
       "      <td>iguaran stated detainees also involved murders police members antikidnapping group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>1</td>\n",
       "      <td>2.400</td>\n",
       "      <td>3 suspected extremists released bail</td>\n",
       "      <td>1 suspected extremist provisionally released without bail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>6 czech hospital employees charged human organ trafficking</td>\n",
       "      <td>accused charged international drug trafficking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>aggravated situation</td>\n",
       "      <td>north korea act aggravate situation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>654 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sim  SimilarityScore  \\\n",
       "0     1            4.000   \n",
       "1     1            5.000   \n",
       "2     1            5.000   \n",
       "3     1            4.667   \n",
       "4     1            4.500   \n",
       "..   ..              ...   \n",
       "649   0            1.800   \n",
       "650   1            3.800   \n",
       "651   1            2.400   \n",
       "652   0            0.800   \n",
       "653   0            2.000   \n",
       "\n",
       "                                                                                           sentence1  \\\n",
       "0                                                                            last year wanted murder   \n",
       "1           promarket economists dont object corporations blatantly use snob appeal promote products   \n",
       "2                                   perhaps importantly ahmadinejad destabilizing influence bernanke   \n",
       "3                                                                                             europe   \n",
       "4                                                       gays modern practices rejected selfindulgent   \n",
       "..                                                                                               ...   \n",
       "649                                indian pakistani governments nearly engaged fourth conflict 1999    \n",
       "650  iguaran stated detainees accused homicide criminal collaboration kidnappings funding terrorism    \n",
       "651                                                            3 suspected extremists released bail    \n",
       "652                                      6 czech hospital employees charged human organ trafficking    \n",
       "653                                                                            aggravated situation    \n",
       "\n",
       "                                                                              sentence2  \n",
       "0                                                              last year sought murder   \n",
       "1              economists companies openly using attractiveness luxury promote products  \n",
       "2                        perhaps important ahmadinejad destabilising influence bernanke  \n",
       "3                                                                                europe  \n",
       "4                                                    gay practical modern rejected laws  \n",
       "..                                                                                  ...  \n",
       "649                       indian pakistani governments conducted nuclear tests may 1998  \n",
       "650  iguaran stated detainees also involved murders police members antikidnapping group  \n",
       "651                           1 suspected extremist provisionally released without bail  \n",
       "652                                      accused charged international drug trafficking  \n",
       "653                                                 north korea act aggravate situation  \n",
       "\n",
       "[654 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word2vec from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    \"\"\"Creates a list of lists containing words from each sentence\n",
    "    \n",
    "    Args:\n",
    "        data: list or dataframe containing text in each cell.\n",
    "    Returns: \n",
    "        A list of lists where the sublists are sentences\n",
    "                and the sublist items are words.\"\"\"\n",
    "        \n",
    "    corpus = []\n",
    "    for content in data:\n",
    "        corpus_temp = nltk.word_tokenize(content)\n",
    "        corpus.append(corpus_temp)\n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(pd.concat([data['sentence1'], data['sentence2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickrs/.conda/envs/pre-work/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "# Importing pre-trained model, updating vocab \n",
    "# and training the model (takes long to run)\n",
    "def pre_trained_w2v(corpus):\n",
    "    '''Function to load the GoogleNews pre-trained word2vec and\n",
    "    train it further on another corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list of lists where the sublists are sentences\n",
    "                and the sublist items are words.\n",
    "    Returns: \n",
    "        A word2vec model with a large vocabulary.\n",
    "    '''\n",
    "    w2v_model_2 = Word2Vec(size=300, min_count=1)\n",
    "    w2v_model_2.build_vocab(corpus)\n",
    "    total_examples = w2v_model_2.corpus_count\n",
    "    w2v_google_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/patrickrs/Documents/Gitlab/revealapp/00_exploration/data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    w2v_model_2.build_vocab([list(w2v_google_model.vocab.keys())], update=True)\n",
    "    w2v_model_2.intersect_word2vec_format('/Users/patrickrs/Documents/Gitlab/revealapp/00_exploration/data/GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)\n",
    "    # intersect_word2vec_format() will let you bring vectors from an external file into a model that's already had its own vocabulary initialized\n",
    "    # see https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.intersect_word2vec_format.html\n",
    "    w2v_model_2.train(corpus, total_examples=total_examples, epochs=w2v_model_2.iter)\n",
    "    return w2v_model_2\n",
    "w2v_model_2 = pre_trained_w2v(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(w2v_model_2.wv.vocab) + 1, 300))\n",
    "for i, vec in enumerate(w2v_model_2.wv.vectors):\n",
    "  embedding_matrix[i] = vec\n",
    "feature_size = 500\n",
    "tokenizer = Tokenizer(num_words = feature_size)\n",
    "# fit the tokenizer on our text\n",
    "tokenizer.fit_on_texts(pd.concat([data['sentence1'], data['sentence2']]))\n",
    "# get all words that the tokenizer knows\n",
    "word_index = tokenizer.word_index\n",
    "# put the tokens in a matrix\n",
    "X1 = tokenizer.texts_to_sequences(data['sentence1'])\n",
    "X1 = pad_sequences(X1)\n",
    "X2 = tokenizer.texts_to_sequences(data['sentence2'])\n",
    "X2 = pad_sequences(X2)\n",
    "\n",
    "# X2 was padded with one column less:\n",
    "x0 = np.zeros((X2.shape[0], X1.shape[1]))\n",
    "x0[:,X1.shape[1] - X2.shape[1]:] = X2\n",
    "X2 = x0\n",
    "\n",
    "X =  np.concatenate((X1, X2), axis = 1)\n",
    "# prepare the labels\n",
    "y = data['sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state = 42)\n",
    "X1_train = X_train[:, :X1.shape[1]]\n",
    "X2_train = X_train[:, X1.shape[1]:X_train.shape[1]]\n",
    "X1_test = X_test[:, :X1.shape[1]]\n",
    "X2_test = X_test[:, X1.shape[1]:X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate schedule\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(X1.shape[1],))\n",
    "input_2 = Input(shape=(X2.shape[1],))\n",
    "\n",
    "\n",
    "common_embed = Embedding(name = \"Sentence_Embed\",\n",
    "                        input_dim = len(w2v_model_2.wv.vocab) + 1 ,\n",
    "                        output_dim = 300,\n",
    "                        input_length  = X1.shape[1],\n",
    "                        weights = [embedding_matrix],\n",
    "                        trainable=False)\n",
    "lstm_1 = common_embed(input_1)\n",
    "lstm_2 = common_embed(input_2)\n",
    "\n",
    "\n",
    "common_lstm = LSTM(256, return_sequences=True, activation=\"relu\")\n",
    "vector_1 = common_lstm(lstm_1)\n",
    "vector_1 = Flatten()(vector_1)\n",
    "\n",
    "vector_2 = common_lstm(lstm_2)\n",
    "vector_2 = Flatten()(vector_2)\n",
    "\n",
    "x3 = Subtract()([vector_1, vector_2])\n",
    "x3 = Multiply()([x3, x3])\n",
    "\n",
    "x1_ = Multiply()([vector_1, vector_1])\n",
    "x2_ = Multiply()([vector_2, vector_2])\n",
    "x4 = Subtract()([x1_, x2_])\n",
    "\n",
    "# https://stackoverflow.com/a/51003359/10650182\n",
    "# Calculates cosine similarity\n",
    "x5 = Dot(axes = 1, normalize=True)([vector_1, vector_2])\n",
    "    \n",
    "conc = Concatenate(axis = -1)([x5, x4, x3])\n",
    "\n",
    "x = Dense(300, activation=\"relu\", name='conc_layer')(conc)\n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
    "\n",
    "model = Model([input_1, input_2], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"binary_crossentropy\", metrics=['acc', keras.metrics.AUC()],\n",
    "              optimizer = SGD(learning_rate=0.01, momentum=0.0, nesterov=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network. input_1 and input_2 are pre-processed, Keras-tokenized text sequences which are to be compared for similar intent. These two text sequences are then fed through a common network of a basic embedding layer and an LSTM units. Once the feature vectors are obtained from this common network, a series of similarity measures are computed and are concatenated to be finally input into a Dense layer followed by sigmoid output unit which will finally help in classifying whether the given texts are similar or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='model.png', show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        restore_best_weights=True,\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-3,\n",
    "        # \"no longer improving\" being further defined as \"for at least 10 epochs\"\n",
    "        patience=30,\n",
    "        verbose=1)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 523 samples, validate on 131 samples\n",
      "Epoch 1/1000\n",
      "523/523 [==============================] - 13s 24ms/sample - loss: 0.6999 - acc: 0.0956 - auc_2: 0.4494 - val_loss: 0.6946 - val_acc: 0.3053 - val_auc_2: 0.5066\n",
      "Epoch 2/1000\n",
      "523/523 [==============================] - 12s 24ms/sample - loss: 0.6956 - acc: 0.2447 - auc_2: 0.5329 - val_loss: 0.6926 - val_acc: 0.6489 - val_auc_2: 0.4875\n",
      "Epoch 3/1000\n",
      "523/523 [==============================] - 6s 11ms/sample - loss: 0.6915 - acc: 0.6750 - auc_2: 0.4701 - val_loss: 0.6906 - val_acc: 0.6947 - val_auc_2: 0.5216\n",
      "Epoch 4/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6869 - acc: 0.8910 - auc_2: 0.4798 - val_loss: 0.6887 - val_acc: 0.6947 - val_auc_2: 0.4875\n",
      "Epoch 5/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6829 - acc: 0.9140 - auc_2: 0.5322 - val_loss: 0.6868 - val_acc: 0.6947 - val_auc_2: 0.5286\n",
      "Epoch 6/1000\n",
      "523/523 [==============================] - 5s 10ms/sample - loss: 0.6790 - acc: 0.9140 - auc_2: 0.5384 - val_loss: 0.6850 - val_acc: 0.6947 - val_auc_2: 0.4945\n",
      "Epoch 7/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6746 - acc: 0.9140 - auc_2: 0.5037 - val_loss: 0.6832 - val_acc: 0.6947 - val_auc_2: 0.5066\n",
      "Epoch 8/1000\n",
      "523/523 [==============================] - 8s 16ms/sample - loss: 0.6709 - acc: 0.9140 - auc_2: 0.5370 - val_loss: 0.6815 - val_acc: 0.6947 - val_auc_2: 0.4945\n",
      "Epoch 9/1000\n",
      "523/523 [==============================] - 6s 11ms/sample - loss: 0.6670 - acc: 0.9140 - auc_2: 0.5243 - val_loss: 0.6798 - val_acc: 0.6947 - val_auc_2: 0.5181\n",
      "Epoch 10/1000\n",
      "523/523 [==============================] - 5s 10ms/sample - loss: 0.6633 - acc: 0.9140 - auc_2: 0.5560 - val_loss: 0.6782 - val_acc: 0.6947 - val_auc_2: 0.4890\n",
      "Epoch 11/1000\n",
      "523/523 [==============================] - 5s 10ms/sample - loss: 0.6596 - acc: 0.9140 - auc_2: 0.5354 - val_loss: 0.6766 - val_acc: 0.6947 - val_auc_2: 0.4717\n",
      "Epoch 12/1000\n",
      "523/523 [==============================] - 5s 10ms/sample - loss: 0.6558 - acc: 0.9140 - auc_2: 0.5983 - val_loss: 0.6751 - val_acc: 0.6947 - val_auc_2: 0.4835\n",
      "Epoch 13/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6528 - acc: 0.9140 - auc_2: 0.5123 - val_loss: 0.6735 - val_acc: 0.6947 - val_auc_2: 0.4863\n",
      "Epoch 14/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6491 - acc: 0.9140 - auc_2: 0.4637 - val_loss: 0.6721 - val_acc: 0.6947 - val_auc_2: 0.4935\n",
      "Epoch 15/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6454 - acc: 0.9140 - auc_2: 0.5099 - val_loss: 0.6706 - val_acc: 0.6947 - val_auc_2: 0.4849\n",
      "Epoch 16/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6420 - acc: 0.9140 - auc_2: 0.4393 - val_loss: 0.6692 - val_acc: 0.6947 - val_auc_2: 0.5106\n",
      "Epoch 17/1000\n",
      "523/523 [==============================] - 5s 9ms/sample - loss: 0.6388 - acc: 0.9140 - auc_2: 0.5197 - val_loss: 0.6679 - val_acc: 0.6947 - val_auc_2: 0.4701\n",
      "Epoch 18/1000\n"
     ]
    }
   ],
   "source": [
    "batch = 523\n",
    "epochs = 1000\n",
    "history = model.fit([X1_train, X2_train], y_train, \n",
    "                     batch, \n",
    "                     epochs = epochs, \n",
    "                     callbacks=callbacks,\n",
    "                     validation_data = ([X1_test, X2_test], y_test)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss (Binary Cross-Entropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss (Binary Cross-Entropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "plt.plot(history.history['val_auc_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X1_test, X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = pyroc.ROC(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "rocc =roc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU better than LSTM? https://www.aclweb.org/anthology/R19-1116.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
