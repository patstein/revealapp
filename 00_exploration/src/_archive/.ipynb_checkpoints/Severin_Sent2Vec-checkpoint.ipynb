{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sent2Vec Exploration\n",
    "Trying to investigate if sent2vec would be a nice tool to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sent2vec installation instructions:\n",
    "- use pip to install cython: pip install cython\n",
    "- clone sent2vec githuh repo: git clone https://github.com/epfml/sent2vec.git\n",
    "- go to the folder and install: pip install .\n",
    "- in jupyter import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import gensim\n",
    "import gzip\n",
    "\n",
    "import sent2vec\n",
    "#from gensim import sent2vec\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/Users/patrickrs/Documents/GitLab/patrick-steiner/revealapp/00_exploration/Pat')\n",
    "#data_original = pd.read_csv('../../../../severin-kappeler/06-NLP/data/job_ads_eng.csv')  # .sample(50000, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = '''I opened up a money market account at First Niagara Bank in XXXX XXXX CT on XXXX/XXXX/2016. I was told that the APY interest rate was 1.01 % based on the advertisement in the XXXX XXXX as well as a verbal discussion. There was no mention that the rate was related to compounding and this was confirmed verbally with the bank officer. And that taking out the interest monthly would not affect the rate that I was to receive. After 33 days of \" interest '', from XX/XX/XXXX to XX/XX/XXXX, I received not XXXX of 1.01 % but rather XXXX of 0.81 % and my printed statement says that my APY is now 0.81 % not the 1.01 % I was expecting. \n",
    "The interest I received was XXXX % LESS that I was expecting. The difference in the amount of money is not very much but First Niagara, in my opinion, lied to me with false advertisements and false verbal discussions, apparently to get me to put my money into a seemingly high interest money market account. Multiply this by XXXX customers and XXXX dollars and the amount of money they defrauded consumers could be very substantial. In my opinion, First Niagara deceived me and its customers and violated honest banking practices.\n",
    "Product: Bank account or service'''\n",
    "doc_2 = '''In this post we covered different approaches for word representation in NLP tasks (BOW, TF-IDF and Word Embeddings), learnt how to learn word representation from its context using Word2Vec, saw how we can extract meaningful phrases from a given corpus (NPMI and data-driven approach) and how to transform a given corpus in order to learn similar terms/words for each one of extracted terms/words using Word2Vec algorithm. The results of this process can be used in a downstream task, like Query Expansion in Information Extraction tasks, Document Classification, Clustering, Question-Answering and many more.'''\n",
    "doc_3 = '''On our 1.6 billion words corpus, it took us 1 hour to construct bi-grams and another 2 hours to train Word2Vec (with batch Skip-Gram, 300 dimension, 10 epochs, context of k=5 , negative sampling of 5, learning rate of 0.01 and minimum word count of 5) on a machine with 16 CPUs and 64 RAM using AWS Sagemaker service. A great Notebook example of how to use AWS Sagemaker service to train Word2Vec can be found here.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = pd.DataFrame(data = [doc_1, doc_2, doc_3], columns=['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Function for cleaning and stemming the data\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = re.sub(\"xxxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xx\", \"\", sentence)\n",
    "    sentence = re.sub(\"\\s\\s+\", \" \", sentence)\n",
    "       \n",
    "    # stemming of words (seems not to affect accuracy, but should make things faster\n",
    "   # porter = PorterStemmer()\n",
    "   # words = word_tokenize(sentence)\n",
    "   # sentence = \" \".join([porter.stem(word) for word in words])\n",
    "      \n",
    "    sentence = sentence.split(\" \")\n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "    sentence = \" \".join(sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data[data['Content'] == data['Content']]  # removes nan since nan == nan -> False\n",
    "    \n",
    "    for col in ['Content']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cleaning and stemming the data\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = re.sub(\"xxxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xx\", \"\", sentence)\n",
    "    sentence = re.sub(\"\\s\\s+\", \" \", sentence)\n",
    "       \n",
    "    # stemming of words (seems not to affect accuracy, but should make things faster\n",
    "   # porter = PorterStemmer()\n",
    "   # words = word_tokenize(sentence)\n",
    "   # sentence = \" \".join([porter.stem(word) for word in words])\n",
    "      \n",
    "    sentence = sentence.split(\" \")\n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "    sentence = \" \".join(sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data[data['Content'] == data['Content']]  # removes nan since nan == nan -> False\n",
    "    \n",
    "    for col in ['Content']:\n",
    "        for row_idx, row in enumerate(data[col]):\n",
    "            for idx, sentence in enumerate(data[col][row_idx]): #data[col][row] is a list of list strings, where each string is a sentence in this doc.\n",
    "                data[col][row_idx][idx] = clean_sentence(sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#splitting data into sentences\n",
    "def tokenize_to_sentences(data):\n",
    "    for col in ['Content']:\n",
    "        data[col] = data[col].apply(nltk.sent_tokenize)\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_to_sentences(sample_text)\n",
    "data = clean_dataframe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opened money market account first niagara bank ct 2016',\n",
       " 'told apy interest rate 101 based advertisement well verbal discussion',\n",
       " 'mention rate related compounding confirmed verbally bank officer',\n",
       " 'taking interest monthly would affect rate receive',\n",
       " '33 days interest received 101 rather 081 printed statement says apy 081 101 expecting',\n",
       " 'interest received less expecting',\n",
       " 'difference amount money much first niagara opinion lied false advertisements false verbal discussions apparently get put money seemingly high interest money market account',\n",
       " 'multiply customers dollars amount money defrauded consumers could substantial',\n",
       " 'opinion first niagara deceived customers violated honest banking practices',\n",
       " 'product bank account service']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably we will need to use some pretrained versions of those which are available in the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sent2vec.Sent2vecModel()\n",
    "#model.embed_sentences(data['Content'][0])\n",
    "#model.load_model('model.bin')\n",
    "#emb = model.embed_sentence(\"once upon a time .\") \n",
    "#embs = model.embed_sentences([\"first sentence .\", \"another sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 0 positional arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a06d5fcbff12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSent2vecModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 0 positional arguments (1 given)"
     ]
    }
   ],
   "source": [
    "model = sent2vec.Sent2vecModel(data['Content'], size=100, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 is trained only on the available data\n",
    "model_1 = word2vec.Word2Vec(corpus, size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-trained model, updating vocab to include only words present in current dataset.\n",
    "# and training the model (takes long to run)\n",
    "model_2 = word2vec.Word2Vec(size=300, min_count=1)\n",
    "model_2.build_vocab(corpus)\n",
    "total_examples = model_2.corpus_count\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/patrickrs/Documents/Gitlab/patrick-steiner/revealapp/Playground/Tag/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model_2.build_vocab([list(model.vocab.keys())], update=True)\n",
    "model_2.intersect_word2vec_format('/Users/patrickrs/Documents/Gitlab/patrick-steiner/revealapp/Playground/Tag/GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)\n",
    "# intersect_word2vec_format() will let you bring vectors from an external file into a model that's already had its own vocabulary initialized\n",
    "# see https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.intersect_word2vec_format.html\n",
    "model_2.train(corpus, total_examples=total_examples, epochs=model_2.iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion2020",
   "language": "python",
   "name": "propulsion2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
