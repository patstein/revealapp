{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author__: Patrick Steiner\n",
    "\n",
    "__Created__: 31.03.2020\n",
    "\n",
    "__Version__: 2\n",
    "\n",
    "__Description__: Creates sentence embedding based on tf-idf and word-embedding and feeds this in a NN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Change ID | Date       |  Author             |Change Description                   |\n",
    "|----------|------------|---------------------|--------------------------------------|\n",
    "|#SK01     | 03.04.2020 | Severin Kappeler    |Re-define corpus with split by sentence rather than by document | \n",
    "|#SK02     | 03.04.2020 | Severin Kappeler    |Embedding matrix weighted by TF-IDF score for each word-embedding vector. Using here the \"global\" TF-IDF. For a given word the \"global\" TF-IDF score is the average of all the TF-IDF scores for that word over all documents.  | \n",
    "|#SK03     | 03.04.2020 | Severin Kappeler    |Apply tokenize.texts_to_sequence to sentence list rather than document list| \n",
    "|#SK04     | 03.04.2020 | Severin Kappeler    |Converting label (target) to be on sentence level rather than document level.      | \n",
    "|#SK05     | 03.04.2020 | Severin Kappeler    |Adding split to sentence part for features |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see also 06-Natural-Language_processing notebooks. This one focuses only on doc2vec.\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import smart_open\n",
    "import collections\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import tqdm\n",
    "import warnings\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.model_selection import GridSearchCV, RandomSearch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/Users/patrickrs/Documents/GitLab/patrick-steiner/Exercises')\n",
    "target = pd.read_csv('.. ../../../../severin-kappeler/06-NLP/data/train_target.csv', index_col = 0)\n",
    "features = pd.read_csv('../../../../severin-kappeler/06-NLP/data/train_features.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[0:1000]\n",
    "target   = target[0:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sevi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#TO DO: Clean the columns (removing missing values)\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = re.sub(\"xxxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xxx\", \"\", sentence)\n",
    "    sentence = re.sub(\"xx\", \"\", sentence)\n",
    "    sentence = re.sub(\"\\s\\s+\", \" \", sentence)\n",
    "       \n",
    "    # stemming of words (seems not to affect accuracy, but should make things faster\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    sentence = \" \".join([porter.stem(word) for word in words])\n",
    "      \n",
    "    sentence = sentence.split(\" \")\n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "    sentence = \" \".join(sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "#SK05\n",
    "#splitting data into sentences\n",
    "def tokenize_to_sentences(data):\n",
    "    output_data = data.copy()\n",
    "    output_data = output_data.apply(nltk.sent_tokenize)\n",
    "        \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime: 3'\n",
    "#create list of all document strings\n",
    "documents = features['Consumer complaint narrative']\n",
    "#create list of document strings split by sentence (a list for each document where each document is represented by a list of sentences (strings))\n",
    "doc_sent = tokenize_to_sentences(documents)\n",
    "#create list of document strings split by cleaned sentences\n",
    "doc_sent_clean = [[clean_sentence(sentence) for sentence in document] for document in doc_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have an installment loan out with Oklahoma C...\n",
       "1    I was told by the Department of Education to c...\n",
       "Name: Consumer complaint narrative, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [I have an installment loan out with Oklahoma ...\n",
       "1    [I was told by the Department of Education to ...\n",
       "Name: Consumer complaint narrative, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sent[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['instal loan oklahoma credit ok call cell phone numer time unknownblock number',\n",
       "  'ask stop simpl fact answer call call result know tri get reach regard loan',\n",
       "  'know cfpb regul compani hide ident'],\n",
       " ['wa told depart educ contact thi collect compani',\n",
       "  'told depart educ collect compani loan belong becaus name chang debt seven year old',\n",
       "  'accord rule fcra debt suppos delet system becaus inaccur']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sent_clean[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'patiencei'  [sentence for doc in doc_sent_clean for sentence in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Corpus: #SK01\n",
    "#runtime: 15''\n",
    "def build_corpus(data):\n",
    "    return [nltk.word_tokenize(sentence) for doc in data for sentence in doc]\n",
    "\n",
    "corpus = build_corpus(doc_sent_clean)\n",
    "\n",
    "#Old version:\n",
    "# def build_corpus(data):\n",
    "#     \"Creates a list of lists containing words from each sentence\"\n",
    "#     corpus = []\n",
    "#     for content in data:\n",
    "#         corpus_temp = nltk.word_tokenize(content)\n",
    "#         corpus.append(corpus_temp)\n",
    "#     return corpus\n",
    "\n",
    "# corpus = build_corpus(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10960"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of sentences overall (can contain duplicates!):\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (Pretrained Google News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sevi/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(470346, 548965)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pre-trained model, updating vocab \n",
    "# and training the model (takes long to run)\n",
    "#runtime without google: 18''\n",
    "model_2 = Word2Vec(size=300, min_count=1, window = 5)\n",
    "model_2.build_vocab(corpus)\n",
    "total_examples = model_2.corpus_count\n",
    "\n",
    "# #for testing commented out:\n",
    "# model_pt = gensim.models.KeyedVectors.load_word2vec_format('/home/sevi/Desktop/Work/PropulsionDS/Pretrained/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model_2.build_vocab([list(model_pt.vocab.keys())], update=True)\n",
    "# model_2.intersect_word2vec_format('/home/sevi/Desktop/Work/PropulsionDS/Pretrained/GoogleNews-vectors-negative300.bin', binary=True, lockf=1.0)\n",
    "\n",
    "# intersect_word2vec_format() will let you bring vectors from an external file into a model that's already had its own vocabulary initialized\n",
    "# see https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.intersect_word2vec_format.html\n",
    "model_2.train(corpus, total_examples=total_examples, epochs=model_2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5586\n",
      "300\n",
      "5586\n"
     ]
    }
   ],
   "source": [
    "#model_2.wv.vectors contains 26'668 vectors of length 300\n",
    "print(len(model_2.wv.vectors))\n",
    "print(len(model_2.wv.vectors[0]))\n",
    "print(len(model_2.wv.vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [' '.join(document) for document in doc_sent_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK02\n",
    "#this function creates the (for now only global) TF-IDF weights for each word.\n",
    "def calc_tf_idf(cleaned_doc_str = None, global_word = True):\n",
    "    #Calculate TF-IDF score for all words in the corpus\n",
    "    tfidf = TfidfVectorizer(\n",
    "          sublinear_tf=True\n",
    "        , min_df=1\n",
    "        , norm='l2'\n",
    "        , encoding='latin-1'\n",
    "        , ngram_range=(1,1)  #considering only 1-grams for now\n",
    "        , token_pattern= '(?u)\\\\b\\\\w+\\\\b' #changed from default in order to also include single letters in case they appear in the corpus.\n",
    "        , tokenizer=nltk.word_tokenize #we pass the corpus as words already.\n",
    "        , stop_words=None #we already removed stopwords\n",
    "    ) \n",
    "    tfidf_matrix = tfidf.fit_transform(cleaned_doc_str)#.toarray()\n",
    "    \n",
    "    if(global_word):\n",
    "        #average all TF-idfs for all documents:\n",
    "        tfidf_matrix = pd.DataFrame(data = tfidf_matrix.mean(axis = -0)[0]\n",
    "                                    , columns = tfidf.get_feature_names()\n",
    "                                    , index = ['TFIDF']\n",
    "                                    )\n",
    "    if(not(len(tfidf.get_feature_names()) == len(model_2.wv.vocab.keys()))):\n",
    "        warnings.warn('The count of vocabulary from the word embedding does not coincide with the TF-IDF Vectorizer features.')\n",
    "    \n",
    "    return tfidf_matrix\n",
    "    #None\n",
    "    #tfidf_matrix contains TF-IDF scores for each document (in the rows) and word (in the columns)\n",
    "    #tfidf.get_feature_names() gives you the words in the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_word_tfidf = calc_tf_idf(doc_clean, global_word = True)\n",
    "#global_word_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5586"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_word_tfidf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Word2Vec & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through tfidf feature_names (the model vocab can be much larger since pretrained. But we are only interested in our own words from the corpus.)\n",
    "embedding_matrix = np.zeros((len(global_word_tfidf.columns) + 1, 300))\n",
    "for i, token in enumerate(global_word_tfidf.columns):\n",
    "  embedding_matrix[i] = model_2.wv.get_vector(token) * global_word_tfidf[token].values[0]\n",
    "#embedding_matrix is 26'668 x 300 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_clean = [sentence for document in doc_sent_clean for sentence in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK03\n",
    "feature_size = 500\n",
    "tokenizer = Tokenizer(num_words = feature_size, split = ' ')\n",
    "# fit the tokenizer on our text\n",
    "tokenizer.fit_on_texts(sent_clean) #SK03 changed to be applied son sent_clean instead of features (which is grouped by docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the tokens in a matrix\n",
    "X = tokenizer.texts_to_sequences(sent_clean)\n",
    "X = pad_sequences(X)\n",
    "#len(X) = number of sentences in all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SK04\n",
    "def calc_target_per_sent(sentence_per_doc = None, target_original = None):\n",
    "    \n",
    "    current_target_idx = 0\n",
    "    new_target = pd.DataFrame()\n",
    "    for doc in sentence_per_doc:\n",
    "        for sentence in doc:\n",
    "            new_target = new_target.append(target.iloc[current_target_idx:current_target_idx+1])\n",
    "            \n",
    "        current_target_idx += 1\n",
    "\n",
    "        \n",
    "    return new_target.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = calc_target_per_sent(doc_sent_clean, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the labels\n",
    "y = pd.get_dummies(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10960\n",
      "10960\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sevi/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 250, 300)          1676100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                3612      \n",
      "=================================================================\n",
      "Total params: 2,400,912\n",
      "Trainable params: 724,812\n",
      "Non-trainable params: 1,676,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "model = Sequential()\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(len(model_2.wv.vocab) + 1 ,\n",
    "                    300,\n",
    "                    input_length  = X.shape[1],\n",
    "                    weights = [embedding_matrix],\n",
    "                    trainable=False))\n",
    "# learn the correlations\n",
    "model.add(LSTM(300,return_sequences=False))\n",
    "model.add(Dense(12,activation=\"softmax\")) \n",
    "# output model skeleton\n",
    "model.summary()\n",
    "model.compile(optimizer=\"nadam\",loss=\"categorical_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sevi/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "9864/9864 [==============================] - 564s 57ms/sample - loss: 2.4463 - acc: 0.1210\n",
      "Epoch 2/50\n",
      "9864/9864 [==============================] - 669s 68ms/sample - loss: 2.4366 - acc: 0.1247\n",
      "Epoch 3/50\n",
      "7552/9864 [=====================>........] - ETA: 2:42 - loss: 2.4368 - acc: 0.1299"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-f24c2a93c5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/propulsion2020/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = 64\n",
    "epochs = 50\n",
    "model.fit(X_train,y_train,batch,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion2020",
   "language": "python",
   "name": "propulsion2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
